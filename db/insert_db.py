#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
import argparse
import os
import json
import hashlib
import MySQLdb
import re
import time
import datetime
import warnings
from OpenSSL import crypto as opssl
from dateutil.parser import parse as parsedate
from publicsuffixlist import PublicSuffixList
# sys.path.insert(1, '../ini')
sys.path.insert(1, 'ini/')
import processing_conf as pconf
# sys.path.insert(1, '../../../trunk/shared/')
sys.path.insert(1, 'shared/')
import progress_bar as pb
import dbconnect as db_lib
import extract_tld_esld
import tls_const
import generic
import geoip
sys.path.insert(1, '../clustering/lib/')
import parse_tls_fprints

# Maybe add it in the config file?
TLS_FPRINTS_PATH = '/home/platon/code/github/tls_malware/'\
                   'clustering/data/tls_fprints/imc18_tls_fprints.db'

# DEFINE TABLE COLUMNS
# Replace all these with a query that gets the DB schema
RUNS = ['FILE_ID', 'TS', 'TS_USEC', 'RETCODE', 'FILE_HASH', 'DURATION',
        'DNSPOLICY', 'NETPOLICY', 'VM', 'SCREENSHOT', 'PARAMETERS',
        'CK_VERSION', 'CK_ID', 'CK_TASK_ID', 'INSTALL', 'INSTALL_RUN_ID']
NET_ADDRESSES = ['ip', 'geo', 'cidr', 'asn', 'as_description']
NET_CONNS = ['run_id', 'conn_hash', 'ts', 'ts_usec', 'uid', 'src_ip',
             'src_port', 'dst_ip', 'dst_port', 'proto', 'service', 'duration',
             'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes',
             'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts',
             'resp_ip_bytes']
NET_MIME_TYPES = ['mime', 'mime_hash']
NET_FILES = ['filetype', 'size', 'md5', 'sha1']
NET_ESLDS = ['esld']
NET_URLS = ['url', 'url_hash']
NET_HTTP_UA = ['ua', 'ua_hash']
NET_HTTP_SERVERS = ['server', 'server_hash']
NET_DOMAINS = ['domain', 'esld_id']
NET_HTTP = ['conn_id', 'run_id', 'ts', 'ts_usec', 'method', 'domain_id',
            'esld_id', 'ua_id', 'ip_id', 'referer_id', 'server_id',
            'url_id', 'location_id', 'req_content_length',
            'resp_content_length', 'status_code', 'req_content_type',
            'req_file_id', 'resp_content_type', 'resp_file_id']
NET_DNS = ['conn_id', 'run_id', 'ts', 'ts_usec', 'trans_id', 'query_id', 
           'qtype', 'rcode', 'aa', 'tc', 'rd', 'ra']
NET_DNS_ANSWERS = ['dns_id', 'type_id', 'ip_id', 'alias_id', 'ttl']
NET_SSL_SIG_ALGS = ['sig_alg']
NET_SSL_EXT_HASHES = ['ext_hash']
NET_SSL_CERTS = ['der_hash', 'pem_hash', 'subject', 'subject_cn', 'subject_o',
                 'subject_ou', 'subject_c', 'subject_st', 'subject_l',
                 'subject_email', 'issuer', 'issuer_cn', 'issuer_o',
                 'issuer_ou', 'issuer_c', 'issuer_st', 'issuer_l',
                 'issuer_email', 'not_valid_before', 'not_valid_after',
                 'key_size', 'sig_alg_id', 'num_ext', 'ext_ca', 'version',
                 'serial_number', 'expired', 'ext_san_num', 'ext_aki_id',
                 'ext_ski_id']
NET_SSL_RESUMPTIONS = ['resumption_value']
NET_SSL_VERSIONS = ['version']
NET_SSL_CIPHERS = ['cipher']
NET_SSL_CHAIN_HASHES = ['chain_hash']
NET_SSL_CIPHERS_HASHES = ['cipher_hash']
NET_SSL_CLIENT_FINGERPRINTS = ['fprint_md5', 'ciphers', 'extensions',
                               'ec_curves', 'ec_point_formats',
                               'client_software']
NET_SSL = ['conn_id', 'run_id', 'ts', 'ts_usec', 'ip_id', 'domain_id',
           'sni_id', 'c_fingerprint_id', 'version_id', 'cipher_id',
           'chain_id', 'leaf_cert_id', 'c_chain_id', 'c_leaf_cert_id',
           'num_server_certs', 'validation_id', 'num_client_certs',
           'num_c_ciphers', 'c_ciphers_hash_id', 'num_c_ext', 'c_ext_hash_id',
           'num_s_ext', 's_ext_hash_id', 'established', 'resumed',
           'fake_resumption', 'c_session_id', 's_session_id',
           'c_session_ticket_hash_id', 's_session_ticket_hash_id']
NET_SSL_CHAIN_PAIRS = ['chain_id', 'cert_idx', 'cert_id']
NET_SSL_C_CHAIN_PAIRS = ['chain_id', 'cert_idx', 'cert_id']


BATCH_LIMIT = 500
# BATCH_LIMIT = 1


def guess_hash(h):
    '''Given a hash string, guess the hash type based on the string length'''
    hlen = len(h)
    if hlen == 32:
        return 'md5'
    elif hlen == 40:
        return 'sha1'
    elif hlen == 64:
        return 'sha256'
    else:
        return None


def get_md5(data):
  m = hashlib.md5()
  m.update(data)
  return m.hexdigest()


def guessquerytype(name):
  if not name: return None
  if name[0] in ["'", '"'] and name[-1] in ["'", '"']:
    return guessquerytype(name[1:-1])
  ipv4 = r'((?:[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[b0-9]{1,3})' +\
       r'(?:\.|:\d{2,5})?)'
  expression_ip = re.compile(ipv4, re.IGNORECASE)
  expression_ipv6 = re.compile(r'^[0-f]{0,4}:[0-f]{0,4}:[0-f]{0,4}:'
                 r'[0-f]{0,4}$')
  expression_url = re.compile(".+\.[a-z]+/[^$]", re.DOTALL)
  expression_domain = re.compile(".+\.[a-z]+")
  if (expression_ip.match(name)):
    guess = "ipv4"
  elif (expression_ipv6.match(name)):
    guess = "ipv6"
  elif (expression_url.match(name)):
    guess = "url"
  elif (expression_domain.match(name)):
    guess = "domain"
  else:
    return "NULL"
  return guess


def validate_config(config, args):
    if args.ifile and not os.path.exists(args.ifile):
        sys.stderr.write('[+] Error: Input file provided does not exist.\n')
        exit(1)
    if args.skip and not os.path.exists(args.skip):
        sys.stderr.write('[+] Error: File in --skip param does not exist.\n')
        exit(1)


def build_in_clause(ids):
    in_clause = "( "
    num = len(ids)
    # Build the IN clause for the SQl query
    for pos, val in enumerate(ids):
        if pos == num-1:
            # Last element
            in_clause += "\'%s\' )" % val
        else:
            # Add it
            in_clause += "\'%s\'," % val
    return in_clause


def get_table_ids(cursor, table, by_field, id_field, values, extra_fields=None,
                  escape=True, dump=False):
    table_ids = {}
    if escape:
        new_values = []
        for vl in values:
            if isinstance(vl, str) or isinstance(vl, unicode):
                new_values.append(MySQLdb.escape_string(vl))
            else:
                new_values.append(vl)
        values = new_values
    if dump:
        for v in values:
            print v
    in_clause = build_in_clause(values)
    if not extra_fields:
        query = "SELECT {}, {} FROM {} WHERE {} IN {}".format(by_field,
                                                       id_field, table,
                                                       by_field, in_clause)
    else:
        query = "SELECT {}, {},".format(by_field, id_field)
        num = len(extra_fields)
        for pos, val in enumerate(extra_fields):
            if pos == num-1:
                # Last element
                query += "{}".format(val)
            else:
                # Add it
                query += "{},".format(val)
        query += "\n FROM {} WHERE {} IN {}".format(table, by_field, in_clause)

    try:
        cursor.execute(query)
    except Exception, e:
        sys.stderr.write("Cannot select from {} table.\n".format(table))
        sys.stderr.write("\tError: {}\n".format(e))
        sys.stderr.write("\tQuery: {}\n".format(query))
        sys.stderr.write("\tValues: {}\n".format(values))
        raise
    else:
        if not extra_fields:
            for en in cursor:
                table_ids[en[by_field]] = en[id_field]
        else:
            for en in cursor:
                table_ids[en[by_field]] = {}
                table_ids[en[by_field]][id_field] = en[id_field]
                for f in extra_fields:
                    table_ids[en[by_field]][f] = en[f]
    return table_ids


def filter_batch(batch, no_run_id_hashes):
    '''Here we filter out:
      - PCAPs for which we have no RUN_ID
      - Bro log entries that are not needed.
         (e.g., traffic from local infrastructure)'''
    remove_pcaps = set()
    if no_run_id_hashes:
        for pos, (fpcap, logs) in enumerate(batch):
            fhash, fhash_type, run_id = parse_pcap_name(fpcap)
            if fhash in no_run_id_hashes:
                remove_pcaps.add(pos)
    for idx in sorted(remove_pcaps, reverse=True):
        del batch[idx]

    #
    # Remove connections to specified endpoints
    #
    for fpcap, en in batch:
        remove_conns = set()
        remove_http = set()
        remove_dns = set()
        remove_ssl = set()
        if 'conn.log' in en.keys():
            for pos, conn_en in enumerate(en['conn.log']):
                if ignore_connection(config, conn_en['id_resp_h'], 
                                     conn_en['id_resp_p'],
                                     conn_en['id_orig_h'],
                                     conn_en['proto']):
                    remove_conns.add(pos)

        if 'http.log' in en.keys():
            for pos, http_en in enumerate(en['http.log']):
                if ignore_connection(config, http_en['id_resp_h'], 
                                     http_en['id_resp_p'],
                                     http_en['id_orig_h'],
                                     None):
                    remove_http.add(pos)

        if 'dns.log' in en.keys():
            for pos, dns_en in enumerate(en['dns.log']):
                if not dns_en['query']:
                    remove_dns.add(pos)
                elif dns_en['query'].lower() in config.ignore_dns_queries:
                    remove_dns.add(pos)
                if ignore_connection(config, dns_en['id_resp_h'], 
                                     dns_en['id_resp_p'],
                                     dns_en['id_orig_h'],
                                     None):
                    remove_dns.add(pos)

        if 'ssl.log' in en.keys():
            for pos, ssl_en in enumerate(en['ssl.log']):
                if ignore_connection(config, ssl_en['id_resp_h'], 
                                     ssl_en['id_resp_p'],
                                     ssl_en['id_orig_h'],
                                     None):
                    remove_ssl.add(pos)

        for idx in sorted(remove_conns, reverse=True):
            del en['conn.log'][idx]
        for idx in sorted(remove_http, reverse=True):
            del en['http.log'][idx]
        for idx in sorted(remove_dns, reverse=True):
            del en['dns.log'][idx]
        for idx in sorted(remove_ssl, reverse=True):
            del en['ssl.log'][idx]

        #
        # Remove entries for files.log
        #
        remove_files = set()
        if 'files.log' in en.keys():
            for pos, file_en in enumerate(en['files.log']):
                if 'X509' in file_en['analyzers']:
                    remove_files.add(pos)
                # For now focus only on HTTP files
                if file_en['source'] != 'HTTP':
                    remove_files.add(pos)
                # Bro may fail to calculate any hash; Ignore those cases
                if not file_en['md5']:
                    remove_files.add(pos)
        for idx in sorted(remove_files, reverse=True):
            del en['files.log'][idx]
    return batch


def ignore_connection(conf, dst_ip, dst_port, orig_ip, proto):

    # If the dst ip is the host of the VM, skip
    if dst_ip == conf.vm_host:
        return True

    # If inter-VM traffic, skip
    if (dst_ip in conf.vm_ips) and (orig_ip in config.vm_ips):
        return True

    # If the protocol is icmp and should be filtered out, skip
    if conf.ignore_icmp and proto == 'icmp':
        return True

    # If dst_ip:dst_port is in ignore_endpoints, skip
    endpoint = '{}:{}'.format(dst_ip, dst_port)
    if endpoint in conf.ignore_endpoints:
        return True
    return False


def prepare_conn_rows(config, run_ids, addr_ids, batch):
    conn_rows = []
    for fpcap, en in batch:
        # if 'conn.log' not in en.keys():
        #     sys.stderr.write('[-] No conn.log for PCAP: {}\n'.format(fpcap))
        #     continue
        for conn_en in en['conn.log']:
            # Create the conn_id
            str_hash = '{}{}{}{}{}'.format(conn_en['id_orig_h'],
                                           conn_en['id_orig_p'],
                                           conn_en['id_resp_h'],
                                           conn_en['id_resp_p'],
                                           conn_en['proto'])
            conn_hash = get_md5(str_hash)

            # Get the conn_id
            fhash, fhash_type, run_id = parse_pcap_name(fpcap)
            run_id = run_ids[fhash]


            # Prepare the row for this entry
            row = [run_id,
                   conn_hash,
                   ':'.join(str(conn_en['ts']).split(':')[:-1]),
                   str(conn_en['ts']).split(':')[-1],
                   conn_en['uid'],
                   addr_ids[conn_en['id_orig_h']],
                   conn_en['id_orig_p'],
                   addr_ids[conn_en['id_resp_h']],
                   conn_en['id_resp_p'],
                   conn_en['proto'],
                   conn_en['service'],
                   conn_en['duration'],
                   conn_en['orig_bytes'],
                   conn_en['resp_bytes'],
                   conn_en['conn_state'],
                   # conn_en['local_orig'],
                   conn_en['missed_bytes'],
                   conn_en['history'],
                   conn_en['orig_pkts'],
                   conn_en['orig_ip_bytes'],
                   conn_en['resp_pkts'],
                   conn_en['resp_ip_bytes']]
                   # conn_en['tunnel_parents']]
            # Clean entries
            row = [db_lib.sclean(field) for field in row]
            conn_rows.append(row)

    return conn_rows


def parse_pcap_name(pcap_fname):
    uid = None
    fields = pcap_fname.split('.pcap')[0].split('_')
    fhash = fields[0]
    if len(fields) == 2:
        uid = fields[1]
    # Verify that it's indeed a proper file hash
    fhash_type = guess_hash(fhash)
    if not fhash_type:
        sys.stderr.write("[-] Cannot extract file hash from PCAP: {}\n".\
                        format(pcap_fname))
    return fhash, fhash_type, uid


def prepare_addr_rows(batch, args):
    addr_rows = []
    addrs = set()
    asn, asd, cc, cidr = None, None, None, None
    # Collect all (unique) addresses
    for fpcap, en in batch:
        if 'conn.log' in en.keys():
            for conn_en in en['conn.log']:
                if ignore_connection(config, conn_en['id_resp_h'], 
                                     conn_en['id_resp_p'],
                                     conn_en['id_orig_h'],
                                     conn_en['proto']):
                    continue
                addrs.add(conn_en['id_orig_h'])
                addrs.add(conn_en['id_resp_h'])
        if 'http.log' in en.keys():
            for http_en in en['http.log']:
                if ignore_connection(config, http_en['id_resp_h'], 
                                     http_en['id_resp_p'],
                                     http_en['id_orig_h'],
                                     None):
                    continue
                addrs.add(http_en['id_orig_h'])
                addrs.add(http_en['id_resp_h'])
        if 'dns.log' in en.keys():
            for dns_en in en['dns.log']:
                if ignore_connection(config, dns_en['id_resp_h'], 
                                     dns_en['id_resp_p'],
                                     dns_en['id_orig_h'],
                                     None):
                    continue
                addrs.add(dns_en['id_orig_h'])
                addrs.add(dns_en['id_resp_h'])
                if not dns_en['qclass_name'] == 'C_INTERNET'\
                  or not dns_en['rcode'] == 0:
                      continue
                if dns_en['answers']:
                    for pos, ans in enumerate(dns_en['answers']):
                        if guessquerytype(ans) == 'ipv4':
                            addrs.add(ans)
        if 'ssl.log' in en.keys():
            for ssl_en in en['ssl.log']:
                if ignore_connection(config, ssl_en['id_resp_h'], 
                                     ssl_en['id_resp_p'],
                                     ssl_en['id_orig_h'],
                                     None):
                    continue
                addrs.add(ssl_en['id_orig_h'])
                addrs.add(ssl_en['id_resp_h'])

    # Get GeoIP info for each IP
    for ip in addrs:
        if args.geoip:
            asn, asd, cc, cidr = geoip.get_geoip_data(ip)
            if not cc or cc == 'IP Address not found' or "can't resolve" in cc:
                asn, asd, cc, cidr = None, None, None, None
        row = [ip, cc, cidr, asn, asd]
        addr_rows.append(row)
    return addr_rows


def prepare_net_files_rows(mime_ids, batch):
    files_rows =[]
    fuids_md5 = {}
    discard = ['-', None]
    # Collect all (unique) addresses
    for fpcap, en in batch:
        if 'files.log' in en.keys():
            for file_en in en['files.log']:
                fuids_md5[file_en['fuid']] = file_en['md5']
                # Prepare row
                if file_en['mime_type'] in discard:
                    mime_id = None
                else:
                    mime_id = mime_ids[file_en['mime_type']]
                row = [mime_id,
                       file_en['seen_bytes'],
                       file_en['md5'],
                       file_en['sha1']]
                files_rows.append(row)
    return files_rows, fuids_md5


def prepare_mime_rows(batch):
    mime_rows = []
    mime_types = set()
    discard = ['-', None]
    for fpcap, en in batch:
        if 'files.log' in en.keys():
            for file_en in en['files.log']:
                if file_en['mime_type'] not in discard:
                    mime_types.add(file_en['mime_type'])
        if 'http.log' in en.keys():
            for http_en in en['http.log']:
                if http_en['req_content_type'] not in discard:
                    mime_types.add(http_en['req_content_type'])
                if http_en['resp_content_type'] not in discard:
                    mime_types.add(http_en['resp_content_type'])
    mime_rows = [[mt, get_md5(mt)] for mt in mime_types]
    return mime_rows


def prepare_secondary_rows(cursor, psl, batch):
    url_rows = []
    ua_rows = []
    server_rows = []
    esld_rows = []
    domain_rows = []
    urls = set()
    uas = set()
    servers = set()
    domains = set()
    eslds = set()
    for fpcap, en in batch:
        if 'http.log' in en.keys():
            for http_en in en['http.log']:
                # Add the urls
                if http_en['referrer']: urls.add(http_en['referrer'])
                if http_en['location']: urls.add(http_en['location'])
                if http_en['method']:
                    if http_en['id_resp_p'] == 80:
                        url = 'http://%s%s' % (http_en['host'], http_en['uri'])
                    else:
                        url = 'http://%s:%s%s' % (http_en['host'],
                                                  http_en['id_resp_p'],
                                                  http_en['uri'])
                    urls.add(url)

                # Add the User Agents
                if http_en['user_agent']: uas.add(http_en['user_agent'])

                # Add the Server (rarely exists)
                if http_en['server']: servers.add(http_en['server'])

                # Add the domain and esld
                if guessquerytype(http_en['host']) == 'domain':
                    domains.add(http_en['host'])
                    # tld, suffix = extract_tld_esld.\
                    #               get_tld_esld(psl, http_en['host'])
                    # eslds.add(suffix)
        if 'dns.log' in en.keys():
            for dns_en in en['dns.log']:
                domains.add(dns_en['query'])
                # tld, suffix = extract_tld_esld.\
                #               get_tld_esld(psl, dns_en['query'])
                # eslds.add(suffix)
                if not dns_en['qclass_name'] == 'C_INTERNET'\
                  or not dns_en['rcode'] == 0:
                      continue
                if dns_en['answers']:
                    for pos, ans in enumerate(dns_en['answers']):
                        if guessquerytype(ans) == 'domain':
                            domains.add(ans)
                            # tld, suffix = extract_tld_esld.\
                            #               get_tld_esld(psl, ans)
                            # eslds.add(suffix)
        if 'ssl_conns.log' in en.keys():
            for ssl_en in en['ssl_conns.log']:
                if ssl_en['dns_domain']:
                    domains.add(ssl_en['dns_domain'])
                    # tld, suffix = extract_tld_esld.\
                    #               get_tld_esld(psl, ssl_en['dns_domain'])
                    # eslds.add(suffix)
                if ssl_en['sni_domain']:
                    domains.add(ssl_en['sni_domain'])
                    # tld, suffix = extract_tld_esld.\
                    #               get_tld_esld(psl, ssl_en['sni_domain'])
                    # eslds.add(suffix)
    # Prepare the rows
    for url in urls:
        url_clean = db_lib.sclean(url)
        url_rows.append([url_clean, get_md5(url_clean)])
    ua_rows = [[ua, get_md5(ua)] for ua in uas]
    server_rows = [[server, get_md5(server)] for server in servers]
    # esld_rows = [[esld.lower()] for esld in eslds]
    esld_rows = []
    for d in domains:
        tld, suffix = extract_tld_esld.get_tld_esld(psl, d)
        esld_rows.append([suffix.lower().strip()])

    # Insert NET_ESLDS
    esld_ids = []
    if esld_rows:
        bulk_insert(cursor, 'NET_ESLDS', NET_ESLDS, esld_rows)
        eslds = [u[0] for u in esld_rows]
        esld_ids = get_table_ids(cursor, 'NET_ESLDS', 'ESLD', 'ESLD_ID', eslds)
    #
    for d in domains:
        if d.lower() == 'null':
            continue
        tld, suffix = extract_tld_esld.get_tld_esld(psl, d)
        suffix = suffix.lower().strip()
        domain_rows.append([d.lower().strip(), esld_ids[suffix]])

    return url_rows, ua_rows, server_rows, domain_rows, esld_ids


def prepare_http_rows(cursor, conn_ids, mime_ids, file_ids, ip_ids, url_ids,
                      ua_ids, server_ids, domain_ids, fuids_md5, batch):
    uids = []
    urls = set()
    http_rows = []
    # Prepare the rows
    for fpcap, en in batch:
        if 'http.log' in en.keys():
            for hl in en['http.log']:
                domain_id = None
                esld_id = None
                ua_id = None
                referrer_id = None
                server_id = None
                url_id = None
                location_id = None
                req_mime_id = None
                resp_mime_id = None
                req_file_id = None
                resp_file_id = None
                if guessquerytype(hl['host']) == 'domain':
                    if hl['host'] != 'null':
                        host = hl['host'].lower().strip()
                        domain_id = domain_ids[host]['DOMAIN_ID']
                        esld_id = domain_ids[host]['ESLD_ID']
                if hl['user_agent']:
                    # ua_clean = MySQLdb.escape_string(hl['user_agent'])
                    ua_id = ua_ids[hl['user_agent']]
                if hl['referrer']:
                    # ref = MySQLdb.escape_string(hl['referrer'])
                    referrer_id = url_ids[hl['referrer']]
                if hl['server']:
                    # server_clean = MySQLdb.escape_string(hl['server'])
                    server_id = server_ids[hl['server']]
                if hl['method']:
                    if hl['id_resp_p'] == 80:
                        url = 'http://%s%s' % (hl['host'], hl['uri'])
                    else:
                        url = 'http://%s:%s%s' % (hl['host'],
                                                  hl['id_resp_p'],
                                                  hl['uri'])
                    # url_clean = MySQLdb.escape_string(url)
                    url_id = url_ids[url]
                if hl['location']:
                    # loc = MySQLdb.escape_string(hl['location'])
                    location_id = url_ids[hl['location']]
                if hl['req_content_type']:
                    req_mime_id = mime_ids[hl['req_content_type']]
                if hl['resp_content_type']:
                    resp_mime_id = mime_ids[hl['resp_content_type']]
                if hl['orig_fuids']:
                    try:
                        req_file_id = file_ids[fuids_md5[hl['orig_fuids'][0]]]
                    except KeyError:
                        # There are cases that a file is mentioned in the HTTP
                        # log but in the files.log we do not have neither
                        # the MD5 nor the SHA1; We ignore those
                        req_file_id = None
                if hl['resp_fuids']:
                    try:
                        resp_file_id = file_ids[fuids_md5[hl['resp_fuids'][0]]]
                    except KeyError:
                        # There are cases that a file is mentioned in the HTTP
                        # log but in the files.log we do not have neither
                        # the MD5 nor the SHA1; We ignore those
                        resp_file_id = None

                row = [conn_ids[hl['uid']]['CONN_ID'],
                       conn_ids[hl['uid']]['RUN_ID'],
                       ':'.join(str(hl['ts']).split(':')[:-1]),
                       str(hl['ts']).split(':')[-1],
                       hl['method'],
                       domain_id,
                       esld_id,
                       ua_id,
                       ip_ids[hl['id_resp_h']],
                       referrer_id,
                       server_id,
                       url_id,
                       location_id,
                       hl['request_body_len'],
                       hl['response_body_len'],
                       hl['status_code'],
                       req_mime_id,
                       req_file_id,
                       resp_mime_id,
                       resp_file_id]
                http_rows.append(row)
    return http_rows, conn_ids


def prepare_dns_rows(conn_ids, domain_ids, batch):
    dns_rows = []
    for fpcap, en in batch:
        if 'dns.log' in en.keys():
            for pos, dns_en in enumerate(en['dns.log']):
                # Set domain id to None if it is a PTR query
                if dns_en['qtype'] == 12 or dns_en['query'].lower() == 'null':
                  domain_id = None
                else:
                  dns_query = dns_en['query'].lower().strip()
                  domain_id = domain_ids[dns_query]['DOMAIN_ID']
                row = [conn_ids[dns_en['uid']]['CONN_ID'],
                       conn_ids[dns_en['uid']]['RUN_ID'],
                       ':'.join(str(dns_en['ts']).split(':')[:-1]),
                       str(dns_en['ts']).split(':')[-1],
                       dns_en['trans_id'],
                       domain_id,
                       dns_en['qtype'],
                       dns_en['rcode'],
                       1 if dns_en['AA'] else 0,
                       1 if dns_en['TC'] else 0,
                       1 if dns_en['RD'] else 0,
                       1 if dns_en['RA'] else 0]
                dns_rows.append(row)

    return dns_rows


def get_conn_ids(batch, cursor):
    '''Use this function to get the conn_id,run_id for any bro connection uid
    that you need. (e.g., fot the HTTP and DNS entries)
    '''
    uids = []
    for fpcap, en in batch:
        # if 'http.log' in en.keys():
        #     for http_en in en['http.log']:
        #         uids.append(http_en['uid'])
        if 'conn.log' in en.keys():
            for conn_en in en['conn.log']:
                uids.append(conn_en['uid'])
        if 'dns.log' in en.keys():
            for dns_en in en['dns.log']:
                uids.append(dns_en['uid'])
    conn_ids = get_table_ids(cursor, 'NET_CONNS', 'UID', 'CONN_ID',
                             uids, ['RUN_ID'])
    return conn_ids


def prepare_dns_answers(conn_ids, domain_ids, dns_ids, ip_ids, batch):
    dns_ans_rows = []
    for fpcap, en in batch:
        if 'dns.log' in en.keys():
            for dns_en in en['dns.log']:
                if not dns_en['qclass_name'] == 'C_INTERNET'\
                  or not dns_en['rcode'] == 0:
                      continue
                if dns_en['answers']:
                    for pos, ans in enumerate(dns_en['answers']):
                        ip_id = None
                        alias_id = None
                        type_id = None
                        if guessquerytype(ans) == 'ipv4':
                          type_id = 1
                          ip_id = ip_ids[ans]
                        elif guessquerytype(ans) == 'domain':
                          type_id = 5
                          ans_clean = ans.lower().strip()
                          alias_id = domain_ids[ans_clean]['DOMAIN_ID']
                        else:
                          continue
                        #Create unique string to identify the dns id
                        if dns_en['qtype'] == 12:
                          domain_id = ''
                        else:
                          dns_query = dns_en['query'].lower().strip()
                          domain_id = domain_ids[dns_query]['DOMAIN_ID']
                        ustring = '{}{}'.format(domain_id, dns_en['ts'])
                        row = [
                               dns_ids[ustring],
                               type_id,
                               ip_id,
                               alias_id,
                               dns_en['TTLs'][pos]]
                        dns_ans_rows.append(row)

    return dns_ans_rows


def get_dns_ids(cursor, conn_ids):
    dns_ids = {}
    in_clause = build_in_clause(conn_ids)
    query = """SELECT DNS_ID, CONN_ID, TS, TS_USEC, QUERY_ID
               FROM NET_DNS
               WHERE CONN_ID IN %s""" % (in_clause)
    for en in cursor:
        table_ids[en[by_field]] = en[id_field]
    try:
        cursor.execute(query)
    except Exception, e:
        sys.stderr.write("Cannot select from NET_DNS table.\n")
        sys.stderr.write("\tError: %s\n" % e)
        sys.stderr.write("\tQuery: %s\n" % query)
        sys.stderr.write("\tValues: %s\n" % conn_ids)
        raise
    else:
        for en in cursor:
            qid = en['QUERY_ID']
            dns_id = en['DNS_ID']
            ts = '{}:{:06d}'.format(en['TS'], en['TS_USEC'])
            if not qid:
                qid = ''
            string = '{}{}'.format(qid, ts)
            dns_ids[string] = dns_id
    return dns_ids


def bulk_insert(cursor, table, columns, values, escape=True):
    if escape:
        new_values = []
        for vl in values:
            if isinstance(vl, str) or isinstance(vl, unicode):
                new_values.append(MySQLdb.escape_string(vl))
            else:
                new_values.append(vl)
        values = new_values
    try:
        # query = """SET SESSION time_zone = '+0:00';\n
        #            INSERT IGNORE INTO {} ({}) VALUES ({})""".\
        cursor.execute("SET SESSION time_zone = '+0:00';")
        query = "INSERT IGNORE INTO {} ({}) VALUES ({})".\
                format(table,
                       ','.join(columns),
                       ','.join(['%s'] * len(columns)))
        cursor.executemany(query, values)
    except Exception, e:
        sys.stderr.write("Cannot bulk insert to {} table.\n".format(table))
        sys.stderr.write("\tError: %s\n" % e)
        sys.stderr.write("\tQuery: %s\n" % query)
        sys.stderr.write("\tValues: %s\n" % values)
        raise


def prepare_cert_rows(conn_ids, batch, resumed_from, args, cursor):
    sig_algs = set()
    ext_hashes = set()
    cert_rows = []
    cert_chains = {}
    for fpcap, en in batch:
        if 'ssl_certs.log' in en.keys():
            for cert_en in en['ssl_certs.log']:
                cert_fname = '{}{}.pem'.\
                             format(args.dcerts, cert_en['DER_hash'])
                cert = opssl.load_certificate(opssl.FILETYPE_PEM,
                                             open(cert_fname).read())

                # Add the cert chains
                try:
                    cert_chains[cert_en['conn_uid']]
                except KeyError:
                    cert_chains[cert_en['conn_uid']] = {'server': [],
                                                        'client': []}
                if 'S' in cert_en['cert_idx']:
                    idx = int(cert_en['cert_idx'].replace('S', ''))
                    cert_chains[cert_en['conn_uid']]['server'].\
                    append((idx, cert_en['DER_hash']))
                elif 'C' in cert_en['cert_idx']:
                    idx = int(cert_en['cert_idx'].replace('C', ''))
                    cert_chains[cert_en['conn_uid']]['client'].\
                    append((idx, cert_en['DER_hash']))

                # Calculate the md5 of the certificate as PEM_hash 
                with open(cert_fname, 'r') as cert_fd:
                    pem_hash = get_md5(cert_fd.read())

                # Complete the fields that are more complex to obtain
                # TODO: Clean this a little? (Copied code)
                alt_domains = []
                ext_ca, auth_key_id, subj_key_id = None, None, None
                for index in range(cert.get_extension_count()):
                  ext_name = str(cert.get_extension(index).\
                                      get_short_name()).strip()

                  if ext_name == 'basicConstraints':
                    ext_ca = cert.get_extension(index).get_critical()
                  #Check for SubjectAltName extension
                  elif ext_name == 'subjectAltName':
                    #Extract all domains icnluded in the extension
                    alt_names = str(cert.get_extension(index)).\
                                         strip('\,').split(',')
                    for name in alt_names:
                      clean_name = name.strip()
                      if "DNS" in clean_name:
                        domain = clean_name.split(':')[1].strip()
                        alt_domains.append(domain.rstrip('.'))
                  elif ext_name == 'authorityKeyIdentifier':
                    try:
                        auth_attrs = str(cert.get_extension(index)).split('\n')
                    except opssl.Error:
                        auth_key_id = None
                    else:
                        for attr in auth_attrs:
                          if "keyid" in attr:
                            auth_key_id = attr.split('keyid:')[1].\
                                    replace(":","").lower()
                  elif ext_name == 'subjectKeyIdentifier':
                    subj_key_id = str(cert.get_extension(index)).\
                            replace(":","").lower()

                # Store stuff that we will need to get their ids
                sig_algs.add(cert.get_signature_algorithm())
                if subj_key_id:
                   ext_hashes.add(subj_key_id)
                if auth_key_id:
                   ext_hashes.add(auth_key_id)

                # Prepare the row
                subj = cert.get_subject()
                iss = cert.get_issuer()
                row = [ cert_en['DER_hash'],
                        pem_hash,
                        cert_en['subject'],
                        subj.CN,
                        subj.O,
                        subj.OU,
                        subj.C,
                        subj.ST,
                        subj.L,
                        subj.emailAddress,
                        cert_en['issuer'],
                        iss.CN,
                        iss.O,
                        iss.OU,
                        iss.C,
                        iss.ST,
                        iss.L,
                        iss.emailAddress,
                        parsedate(cert.get_notBefore()).replace(tzinfo=None),
                        parsedate(cert.get_notAfter()).replace(tzinfo=None),
                        cert.get_pubkey().bits(),
                        # Get the id for this one
                        cert.get_signature_algorithm(),
                        cert.get_extension_count(),
                        ext_ca,
                        cert.get_version() + 1,
                        cert.get_serial_number(),
                        cert.has_expired(),
                        len(alt_domains),
                        # Get the id for this one
                        auth_key_id,
                        # Get the id for this one
                        subj_key_id]

                cert_rows.append(row)
    
    # Insert the signature algorithms
    if sig_algs:
        sig_alg_rows = [[sa] for sa in sig_algs]
        bulk_insert(cursor, 'NET_SSL_SIG_ALGS', NET_SSL_SIG_ALGS, sig_alg_rows)
        sig_alg_ids = get_table_ids(cursor, 'NET_SSL_SIG_ALGS', 'SIG_ALG',
                                    'SIG_ALG_ID', sig_algs)
    # Insert the ext_hashes
    if ext_hashes:
        ext_hashes_rows = [[eh] for eh in ext_hashes]
        bulk_insert(cursor, 'NET_SSL_EXT_HASHES', NET_SSL_EXT_HASHES,
                    ext_hashes_rows)
        ext_hash_ids = get_table_ids(cursor, 'NET_SSL_EXT_HASHES', 'EXT_HASH',
                                    'EXT_HASH_ID', ext_hashes)
    # Update the cert row with the ids obtained
    for row in cert_rows:
        row[21] = sig_alg_ids[row[21]]
        if row[28]:
            row[28] = ext_hash_ids[row[28]]
        if row[29]:
            row[29] = ext_hash_ids[row[29]]
    # Add cert_chain entries for the resumed connections
    for resumed_cuid, info in resumed_from.items():
        cert_chains[resumed_cuid] = cert_chains[info['init_cuid']]
    return cert_rows, cert_chains


def grease_replace(field):
    '''Removes grease codes from field and adds a GREASE string at 
    the end'''
    if not field:
        return field
    clean_field = []
    grease_found = False
    for code in str(field).split(','):
        if int(code) in tls_const.GREASE:
            grease_found = True
            continue
        clean_field.append(code)
    if grease_found:
        clean_field.append('GREASE')
        return ','.join(clean_field)
    else:
        return field


def fp_clean(value):
    # Treat all Bro 'empty' values in the same manner
    if '(empty)' == value or '-' == value:
        return ''
    else:
        return value


def generate_tls_fprint(sslrec):
    '''Generates a TLS fingerprint from a TLS record '''
    if not sslrec['c_ciphers'] and\
       not sslrec['c_exts'] and\
       not sslrec['c_curves'] and\
       not sslrec['c_point_formats']:
           return None
    tls_fprint = '{};{};{};{}'.\
      format(grease_replace(fp_clean(sslrec['c_ciphers'])),
             grease_replace(fp_clean(sslrec['c_exts'])),
             grease_replace(fp_clean(sslrec['c_curves'])),
             grease_replace(fp_clean(sslrec['c_point_formats'])))
    m = hashlib.md5()
    m.update(tls_fprint)
    tls_fp_md5 = m.hexdigest()
    return (tls_fprint, tls_fp_md5)


def get_resumed_conns(batch):
    resumed_from = {}
    all_res_ids = []
    server_session_ids = {}
    server_session_tickets = {}
    fake_resumed_conns = []
    established = {}
    for fpcap, pcap_en in batch:
        if 'ssl_dm.log' in pcap_en.keys():
            # Do first pass to fill the cache with the server side resumption
            # info
            for en in pcap_en['ssl_dm.log']:
                established[en['conn_uid']] = en['established']
                if not en['resumed']:
                    if en['s_session_id']:
                        server_session_ids[en['s_session_id']] = {
                          'init_cuid': en['conn_uid'],
                          'val_error_str': en['s_leaf_cert_validation_status']}
                    if en['s_session_ticket']:
                        server_session_tickets[en['s_session_ticket']] ={
                          'init_cuid': en['conn_uid'],
                          'val_error_str': en['s_leaf_cert_validation_status']}
                # We do this because we want to store the session id or 
                # session ticket hash even if there are fake resumptions
                if en['c_session_id']:
                    all_res_ids.append(en['c_session_id'])
                if en['c_session_ticket']:
                    all_res_ids.append(en['c_session_ticket'])
                if en['s_session_id']:
                    all_res_ids.append(en['s_session_id'])
                if en['s_session_ticket']:
                    all_res_ids.append(en['s_session_ticket'])
            for en in pcap_en['ssl_dm.log']:
                if en['resumed']:
                    # Map the resumed connection with the initial one
                    if en['c_session_id']:
                        try:
                            resumed_from[en['conn_uid']] =\
                                    server_session_ids[en['c_session_id']]
                        except KeyError:
                            fake_resumed_conns.append(en['conn_uid'])
                    else:
                        try:
                            resumed_from[en['conn_uid']] =\
                                 server_session_tickets[en['c_session_ticket']]
                        except KeyError:
                            fake_resumed_conns.append(en['conn_uid'])
    # all_ids = set(server_session_ids.keys() + server_session_tickets.keys() +\
    #               client_session_ids + client_session_tickets)
    resumed_ids = [[x] for x in all_res_ids]
    return resumed_from, fake_resumed_conns, resumed_ids, established


def prepare_ssl_rows(conn_ids, cert_ids, cert_chains, ip_ids, domain_ids,
                     ssl_val_ids, resumed_from, is_established, fake_resumed,
                     resumption_db_ids, tls_fprints, batch, cursor):
    chain_hashes = set()
    tls_versions = set()
    ciphers = set()
    # val_errors = set()
    tls_fps = set()
    ciphers_hashes = set()
    extensions_hashes = set()
    ssl_rows = []
    for fpcap, en in batch:
        if 'ssl_conns.log' in en.keys():
            for ssl_en in en['ssl_conns.log']:
                cleaf_cert = None
                sleaf_cert = None
                cchain = None
                schain = None
                cchain_hash = None
                schain_hash = None
                leaf_scert_id = None
                leaf_ccert_id = None
                sni_domain_id = None
                dns_domain_id = None
                ssl_val_id = None
                fake_resumption = False

                # Get the ordered server certificate chain
                # print cert_chains
                # pprint_dict(ssl_en)
                try:
                    schain = cert_chains[ssl_en['conn_uid']]['server']
                    schain = sorted(schain, key=lambda v: v[0])
                except KeyError:
                    # In the case of non established connections the certs
                    # may not appear
                    if not is_established[ssl_en['conn_uid']] or\
                      ssl_en['conn_uid'] in fake_resumed:
                        pass
                    else:
                        print ssl_en['conn_uid']
                        raise

                # Get the ordered client certificate chain
                try:
                    cchain = cert_chains[ssl_en['conn_uid']]['client']
                    cchain = sorted(cchain, key=lambda v: v[0])
                except KeyError:
                    # In the case of non established connections the certs
                    # may not appear
                    if not is_established[ssl_en['conn_uid']] or\
                      ssl_en['conn_uid'] in fake_resumed:
                        pass
                    else:
                        print ssl_en['conn_uid']
                        raise

                if schain:
                    schain_hash = get_md5(str(''.join([v[1] for v in schain])))
                    chain_hashes.add(schain_hash)
                    leaf_scert_id = cert_ids[schain[0][1]]
                if cchain:
                    cchain_hash = get_md5(str(''.join([v[1] for v in cchain])))
                    chain_hashes.add(cchain_hash)
                    leaf_ccert_id = cert_ids[cchain[0][1]]

                if ssl_en['version']:
                    tls_versions.add(ssl_en['version'])
                if ssl_en['cipher']:
                    ciphers.add(ssl_en['cipher'])

                tls_fp  = generate_tls_fprint(ssl_en)
                if tls_fp:
                    tls_fps.add(tls_fp)

                if ssl_en['c_ciphers_hash']:
                    ciphers_hashes.add(ssl_en['c_ciphers_hash'])
                if ssl_en['c_ext_hash']:
                    extensions_hashes.add(ssl_en['c_ext_hash'])
                if ssl_en['s_ext_hash']:
                    extensions_hashes.add(ssl_en['s_ext_hash'])

                if ssl_en['sni_domain'] and ssl_en['sni_domain'] != 'NULL':
                    sni = ssl_en['sni_domain'].lower().strip()
                    sni_domain_id = domain_ids[sni]['DOMAIN_ID']
                if ssl_en['dns_domain'] and ssl_en['dns_domain'] != 'NULL':
                    dns_domain = ssl_en['dns_domain'].lower().strip()
                    dns_domain_id = domain_ids[dns_domain]['DOMAIN_ID']

                # Fill the certification validation info for resumed conns
                if ssl_en['resumed'] and\
                  ssl_en['conn_uid'] not in fake_resumed:
                    val_error_str =\
                            resumed_from[ssl_en['conn_uid']]['val_error_str']
                    ssl_val_id = ssl_val_ids[val_error_str]
                else:
                    if is_established[ssl_en['conn_uid']] and\
                      ssl_en['conn_uid'] not in fake_resumed:
                        val_error_str = ssl_en['validation_error_str']
                        ssl_val_id = ssl_val_ids[val_error_str]

                if ssl_en['conn_uid'] in fake_resumed:
                    fake_resumption = True

                # Prepare the row
                try:
                    row = [
                            conn_ids[ssl_en['conn_uid']]['CONN_ID'],
                            conn_ids[ssl_en['conn_uid']]['RUN_ID'],
                            ':'.join(str(ssl_en['ts']).split(':')[:-1]),
                            str(ssl_en['ts']).split(':')[-1],
                            ip_ids[ssl_en['dst_IP']],
                            dns_domain_id,
                            sni_domain_id,
                            # Add TLS fingerprint id
                            tls_fp,
                            # Add version id
                            ssl_en['version'],
                            # Add cipher id
                            ssl_en['cipher'],
                            # Add server chain id
                            schain_hash,
                            leaf_scert_id,
                            # Add client chain id
                            cchain_hash,
                            leaf_ccert_id,
                            ssl_en['num_server_certs'],
                            ssl_val_id,
                            ssl_en['num_client_certs'],
                            ssl_en['num_c_ciphers'],
                            # Add client supported ciphers hash id
                            ssl_en['c_ciphers_hash'],
                            ssl_en['num_c_ext'],
                            # Add client extensions hash id
                            ssl_en['c_ext_hash'],
                            ssl_en['num_s_ext'],
                            # Add server extensions hash id
                            ssl_en['s_ext_hash'],
                            is_established[ssl_en['conn_uid']],
                            ssl_en['resumed'],
                            fake_resumption,
                            resumption_db_ids[ssl_en['c_session_id']] \
                                    if ssl_en['c_session_id'] else None,
                            resumption_db_ids[ssl_en['s_session_id']]\
                                    if ssl_en['s_session_id'] else None,
                            resumption_db_ids[ssl_en['c_session_ticket']]\
                                    if ssl_en['c_session_ticket'] else None,
                            resumption_db_ids[ssl_en['s_session_ticket']]\
                                    if ssl_en['s_session_ticket'] else None]
                except:
                    print ssl_en['conn_uid']
                    print fake_resumption
                    pprint_dict(resumption_db_ids)
                    raise
                ssl_rows.append(row)

    # Store SSL versions and get IDs
    ciphers_ids = []
    chain_hash_ids = []
    ciphers_hashes_ids = []
    ext_hash_ids = []
    tls_fps_ids = []
    if tls_versions:
        tls_ver_rows = [[ver] for ver in tls_versions]
        bulk_insert(cursor, 'NET_SSL_VERSIONS', NET_SSL_VERSIONS, tls_ver_rows)
        tls_version_ids = get_table_ids(cursor, 'NET_SSL_VERSIONS',
                                        'VERSION', 'VERSION_ID', tls_versions)
    # Store cipher used for the current SSL connection and get its IDs
    if ciphers:
        cipher_rows = [[c] for c in ciphers]
        bulk_insert(cursor, 'NET_SSL_CIPHERS', NET_SSL_CIPHERS, cipher_rows)
        ciphers_ids = get_table_ids(cursor, 'NET_SSL_CIPHERS',
                                        'CIPHER', 'CIPHER_ID', ciphers)
    # Store the server and client certificate chains and get their IDs
    if chain_hashes:
        chain_hashes_rows = [[sc] for sc in chain_hashes]
        bulk_insert(cursor, 'NET_SSL_CHAIN_HASHES',
                    NET_SSL_CHAIN_HASHES, chain_hashes_rows)
        chain_hash_ids = get_table_ids(cursor, 'NET_SSL_CHAIN_HASHES',
                                'CHAIN_HASH', 'CHAIN_HASH_ID', chain_hashes)
    # Store the TLS fingerprint hashes and get their IDs
    if tls_fps:
        # tls_fps_rows = [[tlh] + tl.split(';') for tl, tlh in tls_fps]
        tls_fps_rows = []
        for tl, tlh in tls_fps:
            try:
                tls_fp_label = tls_fprints[tlh]['desc']
            except KeyError:
                tls_fp_label = None
            row = [tlh] + tl.split(';') + [tls_fp_label]
            tls_fps_rows.append(row)
        bulk_insert(cursor, 'NET_SSL_CLIENT_FINGERPRINTS',
                    NET_SSL_CLIENT_FINGERPRINTS, tls_fps_rows)
        tls_fps_ids = get_table_ids(cursor, 'NET_SSL_CLIENT_FINGERPRINTS',
                                    'FPRINT_MD5', 'FPRINT_ID',
                                    [tlh for tl, tlh in tls_fps])
    # Store the hash of all supported ciphers and get their IDs
    if ciphers_hashes:
        ciphers_hashes_rows = [[ch] for ch in ciphers_hashes]
        bulk_insert(cursor, 'NET_SSL_CIPHERS_HASHES',
                    NET_SSL_CIPHERS_HASHES, ciphers_hashes_rows)
        ciphers_hashes_ids = get_table_ids(cursor, 'NET_SSL_CIPHERS_HASHES',
                                           'CIPHER_HASH', 'CIPHER_HASH_ID',
                                           ciphers_hashes)
    # Store the hash of all extensions and get their IDs
    if extensions_hashes:
        ext_hashes_rows = [[eh] for eh in extensions_hashes]
        bulk_insert(cursor, 'NET_SSL_EXT_HASHES', NET_SSL_EXT_HASHES,
                    ext_hashes_rows)
        ext_hash_ids = get_table_ids(cursor, 'NET_SSL_EXT_HASHES', 'EXT_HASH',
                                    'EXT_HASH_ID', extensions_hashes)

    # Update the missing ids for each row
    for r in ssl_rows:
        if r[7]:
            r[7] = tls_fps_ids[r[7][1]]
        if r[8]:
            r[8] = tls_version_ids[r[8]]
        if r[9]:
            r[9] = ciphers_ids[r[9]]
        if r[10]:
            r[10] = chain_hash_ids[r[10]]
        if r[12]:
            r[12] = chain_hash_ids[r[12]]
        if r[18]:
            r[18] = ciphers_hashes_ids[r[18]]
        if r[20]:
            r[20] = ext_hash_ids[r[20]]
        if r[22]:
            r[22] = ext_hash_ids[r[22]]

    return ssl_rows, chain_hash_ids


def prepare_chain_pairs(chain_hash_ids, cert_chains, cert_ids):
    client_rows = []
    server_rows = []
    for conn_id, en in cert_chains.items():
        if en['server']:
            schain = sorted(en['server'], key=lambda v: v[0])
            schain_hash = get_md5(str(''.join([v[1] for v in schain])))
            for c_idx, c in en['server']:
                row = [
                          chain_hash_ids[schain_hash],
                          c_idx,
                          cert_ids[c]
                      ]
                server_rows.append(row)
        if en['client']:
            cchain = sorted(en['client'], key=lambda v: v[0])
            cchain_hash = get_md5(str(''.join([v[1] for v in cchain])))
            for c_idx, c in en['client']:
                row = [
                          chain_hash_ids[cchain_hash],
                          c_idx,
                          cert_ids[c]
                      ]
                client_rows.append(row)
    return server_rows, client_rows


def prepare_resumption_rows(batch):
    if ext_hashes:
        ext_hashes_rows = [[eh] for eh in ext_hashes]
        bulk_insert(cursor, 'NET_SSL_EXT_HASHES', NET_SSL_EXT_HASHES,
                    ext_hashes_rows)
        ext_hash_ids = get_table_ids(cursor, 'NET_SSL_EXT_HASHES', 'EXT_HASH',
                                    'EXT_HASH_ID', ext_hashes)


def process_batch(cursor, config, run_ids, psl, batch, args,
                  ssl_val_ids, tls_fprints):
    files_ids = []
    ip_ids = []
    mime_ids = []
    url_ids = []
    ua_ids = []
    server_ids = []
    esld_ids = []
    domain_ids = []
    cert_ids = []

    # Insert all IP to NET_ADDRESSES
    addr_rows = prepare_addr_rows(batch, args)
    if addr_rows:
        # insert_net_addresses(cursor, addr_rows)
        bulk_insert(cursor, 'NET_ADDRESSES', NET_ADDRESSES, addr_rows)
        addrs = [f[0] for f in addr_rows]
        ip_ids = get_table_ids(cursor, 'NET_ADDRESSES', 'IP', 'IP_ID', addrs)

    # Insert to CONNs table
    conn_rows = prepare_conn_rows(config, run_ids, ip_ids, batch)
    if conn_rows:
        # insert_conns(cursor, conn_rows)
        bulk_insert(cursor, 'NET_CONNS', NET_CONNS, conn_rows)

    # Insert MIME_TYPES
    mime_rows = prepare_mime_rows(batch)
    if mime_rows:
        bulk_insert(cursor, 'NET_MIME_TYPES', NET_MIME_TYPES, mime_rows)
        mime_types = [mt[0] for mt in mime_rows]
        mime_ids = get_table_ids(cursor, 'NET_MIME_TYPES', 'MIME',
                                 'MIME_ID', mime_types)

    # Insert to NET_FILES tables
    files_rows, fuids_md5 = prepare_net_files_rows(mime_ids, batch)
    if files_rows:
        # insert_net_files(cursor, files_rows)
        bulk_insert(cursor, 'NET_FILES', NET_FILES, files_rows)
        files_md5 = [f[2] for f in files_rows]
        files_ids = get_table_ids(cursor, 'NET_FILES', 'MD5',
                                  'FILE_ID', files_md5)

    # Prepare all info extracted from HTTP.LOG
    # For Tables: NET_URLs, NET_HTTP_UA,
    #             NET_HTTP_SERVERS, NET_DOMAINS, NET_ESLDS,
    #             NET_HTTP
    url_rows, ua_rows, server_rows,\
    domain_rows, esld_ids = prepare_secondary_rows(cursor, psl, batch)

    # Insert to NET_URLs
    if url_rows:
        # insert_net_urls(cursor, url_rows)
        bulk_insert(cursor, 'NET_URLS', NET_URLS, url_rows)
        urls = [u[0] for u in url_rows]
        try:
            url_ids = get_table_ids(cursor, 'NET_URLS', 'URL', 'URL_ID', urls)
        except:
            get_table_ids(cursor, 'NET_URLS', 'URL', 'URL_ID', urls, dump=True)


    # Insert to NET_HTTP_UA
    if ua_rows:
        bulk_insert(cursor, 'NET_HTTP_UA', NET_HTTP_UA, ua_rows)
        uas = [u[0]  for u in ua_rows]
        ua_ids = get_table_ids(cursor, 'NET_HTTP_UA', 'UA', 'UA_ID', uas)

    # Insert to NET_HTTP_SERVERS
    if server_rows:
        bulk_insert(cursor, 'NET_HTTP_SERVERS', NET_HTTP_SERVERS, server_rows)
        servers = [s[0] for s in server_rows]
        server_ids = get_table_ids(cursor, 'NET_HTTP_SERVERS',
                                   'SERVER', 'SERVER_ID', servers)

    # Inser to NET_DOMAINS
    if domain_rows:
        # insert_net_domains(cursor, domain_rows)
        bulk_insert(cursor, 'NET_DOMAINS', NET_DOMAINS, domain_rows)
        domains = [s[0] for s in domain_rows]
        domain_ids = get_table_ids(cursor, 'NET_DOMAINS',
                                   'DOMAIN', 'DOMAIN_ID', domains, ['ESLD_ID'])

    # Get the conn_ids for all conns in http 
    conn_ids = get_conn_ids(batch, cursor)
    # Insert to NET_HTTP tables
    http_rows, conn_ids = prepare_http_rows(cursor, conn_ids, mime_ids,
                                  files_ids, ip_ids, url_ids, ua_ids,
                                  server_ids, domain_ids, fuids_md5, batch)
    if http_rows:
        # insert_http(cursor, http_rows)
        bulk_insert(cursor, 'NET_HTTP', NET_HTTP, http_rows)

    # Insert to NET_DNS
    dns_rows = prepare_dns_rows(conn_ids, domain_ids, batch)
    if dns_rows:
        # insert_dns(cursor, dns_rows)
        bulk_insert(cursor, 'NET_DNS', NET_DNS, dns_rows)
        dns_conn_ids = set([s[0] for s in dns_rows])
        dns_ids = get_dns_ids(cursor, dns_conn_ids)
        dns_answers_rows = prepare_dns_answers(conn_ids, domain_ids, dns_ids,
                                               ip_ids, batch)
        if dns_answers_rows:
            # insert_dns_answers_rows(cursor, dns_answers_rows)
            bulk_insert(cursor, 'NET_DNS_ANSWERS',
                        NET_DNS_ANSWERS, dns_answers_rows)

    # Create the mapping between resumed connections and initial connection
    resumed_from, fake_resum_conns, resumed_ids, established_state =\
            get_resumed_conns(batch)

    # Insert to NET_SSL_RESUMPTION
    resumption_db_ids = []
    if resumed_ids:
        bulk_insert(cursor, 'NET_SSL_RESUMPTIONS',
                    NET_SSL_RESUMPTIONS, resumed_ids)
        resumed_ids_li = [c[0] for c in resumed_ids]
        resumption_db_ids = get_table_ids(cursor, 'NET_SSL_RESUMPTIONS',
                                          'RESUMPTION_VALUE', 'ID',
                                          resumed_ids_li)

    # Insert to NET_CERTS
    cert_rows, cert_chains = prepare_cert_rows(conn_ids, batch, resumed_from,
                                               args, cursor)
    if cert_rows:
        bulk_insert(cursor, 'NET_SSL_CERTS', NET_SSL_CERTS, cert_rows)
        der_hashes = [c[0] for c in cert_rows]
        cert_ids = get_table_ids(cursor, 'NET_SSL_CERTS', 'DER_HASH',
                                'CERT_ID', der_hashes)


    # Insert to NET_SSL
    ssl_rows, chain_hash_ids = prepare_ssl_rows(conn_ids,
                                    cert_ids, cert_chains, ip_ids, domain_ids,
                                    ssl_val_ids, resumed_from, 
                                    established_state, fake_resum_conns,
                                    resumption_db_ids, tls_fprints, batch,
                                    cursor)
    if ssl_rows:
        bulk_insert(cursor, 'NET_SSL', NET_SSL, ssl_rows)
        # Add the client/server certificate chains
        server_pairs, client_pairs = prepare_chain_pairs(chain_hash_ids,
                                                         cert_chains, cert_ids)
        if server_pairs:
            bulk_insert(cursor, 'NET_SSL_CHAIN_PAIRS',
                        NET_SSL_CHAIN_PAIRS, server_pairs)
        if client_pairs:
            bulk_insert(cursor, 'NET_SSL_C_CHAIN_PAIRS',
                        NET_SSL_C_CHAIN_PAIRS, client_pairs)


def skip(pcap_entry, to_skip):
    if not to_skip:
        return False
    try:
        to_skip[pcap_entry[0]]
    except KeyError:
        return False
    else:
        return True


def update_processed_entries(fname, batch):
    with open(fname, 'a') as fw:
        for fpcap, logs in batch:
            fw.write('{}\n'.format(fpcap))


def get_run_timestamps(batch):
    run_ts = {}
    for fpcap, logs in batch:
        all_ts = []
        for conn_en in logs['conn.log']:
            ts = datetime.datetime.strptime(conn_en['ts'],
                                            "%Y-%m-%d %H:%M:%S:%f")
            all_ts.append(ts)
        fhash, fhash_type, run_id = parse_pcap_name(fpcap)
        run_ts[fhash] = min(all_ts).strftime("%Y-%m-%d %H:%M:%S:%f")
    return run_ts


def pprint_dict(dic):
    '''Pretty prints a dictionary'''
    import pprint
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(dic)


def get_run_ids(cursor, args, batch):
    # pprint_dict(batch)
    # Get pcap hash from file name, if proceeds
    fhashes = []
    no_run_id_hashes = None
    for fpcap, logs in batch:
        fhash, fhash_type, run_id = parse_pcap_name(fpcap)
        fhashes.append(fhash)
    if args.norun:
        htype = generic.guess_hash(fhashes[0])
        file_ids = get_table_ids(cursor, 'FILES', htype, 'FILE_ID', fhashes)
        # Get the timestamp of the first connection in each PCAP
        run_ts = get_run_timestamps(batch)
        run_rows = []
        for fh in fhashes:
            row = [file_ids[fh], ':'.join(run_ts[fh].split(':')[:-1]), 
                   run_ts[fh].split(':')[-1], None,
                   fh] + [None] * 11
            run_rows.append(row)
        # Insert the hashes 
        bulk_insert(cursor, 'RUNS', RUNS, run_rows)
        run_ids = get_table_ids(cursor, 'RUNS', 'FILE_HASH', 'RUN_ID', fhashes)
    else:
        # Get them
        run_ids = get_table_ids(cursor, 'RUNS', 'FILE_HASH', 'RUN_ID', fhashes)
        if len(fhashes) != len(run_ids):
            no_run_id_hashes = set(fhashes).difference(set(run_ids.keys()))
            sys.stderr.write('[+] Ignored {} PCAPs with no RUN_ID.\n'.\
                             format(len(no_run_id_hashes)))

    return run_ids, no_run_id_hashes


def main(args, conf):
    """ """
    # Treat MySQL warnings as errors
    # warnings.filterwarnings('error', category=MySQLdb.Warning)

    # Connect to MySQL DB
    cursor, conn = db_lib.ConnectDB(conf.db_host, conf.db_name,
                                    conf.db_user, conf.db_pass,
                                    sql_mode='TRADITIONAL')

    # This will actually disable the silent truncate feature of MySQL
    # For any field that is truncated it will now generate an error
    sys.stderr.write("[+] Connected to db: %s as user: %s\n" % (conf.db_name,
                                                                conf.db_user))

    # Get TLS fingerprints
    tls_fprint_db = parse_tls_fprints.TLSFingerprintDB(TLS_FPRINTS_PATH)
    tls_fprints = tls_fprint_db.get_contents()
    sys.stderr.write('[-] Parsed known TLS fingerprints\n')

    # Initialize Public Suffix for domain to ESLD conversion
    psl = PublicSuffixList()

    # Get total number of lines
    total_cnt = pb.num_file_lines(args.ifile)

    # Get all SSL validation errors from DB
    ssl_val_ids = {}
    query = """SELECT VAL_ID, VAL FROM NET_SSL_VALIDATION"""
    try:
        cursor.execute(query)
    except Exception, e:
        sys.stderr.write("Cannot select from NET_SSL_VALIDATION table.\n")
        sys.stderr.write("\tError: %s\n" % e)
        sys.stderr.write("\tQuery: %s\n" % query)
        raise
    for en in cursor:
        ssl_val_ids[en['VAL']] = int(en['VAL_ID'])

    # Get PCAPs to skip
    to_skip = {}
    if args.skip:
        with open(args.skip, 'r') as fr:
            for pos, line in enumerate(fr):
                to_skip[line.strip('\n')] = None

    #Create name of file to store processed pcaps
    processed_fname = 'processed_{}'.format(time.time())

    batch = []
    pb.update_progress_bar(0, total_cnt)
    with open(args.ifile) as fr:
        for pos, line in enumerate(fr):
            if batch and len(batch) % BATCH_LIMIT == 0:
                pb.update_progress_bar(pos, total_cnt)
                # Get RUN_IDs
                run_ids, no_run_id_hashes = get_run_ids(cursor, args, batch)
                batch = filter_batch(batch, no_run_id_hashes)
                if batch:
                    process_batch(cursor, config, run_ids, psl, batch,
                                  args, ssl_val_ids, tls_fprints)
                    # Update processed files
                    update_processed_entries(processed_fname, batch)
                    conn.commit()
                    batch = []
            pcap_entry = json.loads(line.strip('\n'))
            # Check if we the entry is already processed
            if to_skip and skip(pcap_entry, to_skip):
                continue
            # If there are no connections in the conn.log ignore it
            if 'conn.log' not in pcap_entry[2].keys():
                sys.stderr.write('[-] Ignored PCAP with no conn.log: {}\n'.\
                                 format(pcap_entry[0]))
                continue
            # Put in batch only pcaps for which there was traffic
            if pcap_entry[2]:
                pentry = [pcap_entry[0], pcap_entry[2]]
                # batch.append(pcap_entry)
                batch.append(pentry)
            else:
                sys.stderr.write('[-] Empty PCAP: {}\n'.format(pcap_entry[0]))
    if batch:
        pb.update_progress_bar(pos, total_cnt)
        # Get RUN_IDs
        run_ids, no_run_id_hashes = get_run_ids(cursor, args, batch)
        batch = filter_batch(batch, no_run_id_hashes)
        process_batch(cursor, config, run_ids, psl, batch, args,
                      ssl_val_ids, tls_fprints)
        # Update processed files
        update_processed_entries(processed_fname, batch)
        conn.commit()


if __name__ == '__main__':
    # Parse configuration file
    config = pconf.ProcessingConfig()

    # Process parameters
    argparser = argparse.ArgumentParser(prog='insert_db',
                      description=\
          '''Given a list of JSONs with Bro Logs it will populate them in 
          the DB provided in the configuration file
          bro and insert them into a database'''.replace('\n', ' '))
    argparser.add_argument('--ifile', help='file with JSONS', required=True)
    argparser.add_argument('--dcerts', help='directory with certs',
                           required=True)
    argparser.add_argument('--norun',
                           help='Flag to use if no run info is available. '\
                                'Run ids generated automatically.',
                           action='store_true', default=False)
    argparser.add_argument('--skip',
                           help='File with already processed pcaps')
    argparser.add_argument('--geoip',
                           help='Does Geolocation of all IPs found'\
                                'Slows down the process.',
                           action='store_true', default=False)
    args = argparser.parse_args()
    validate_config(config, args)
    if not args.dcerts[-1] == '/':
        args.dcerts += '/' 
    main(args, config)

