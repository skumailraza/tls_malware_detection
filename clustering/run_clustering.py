import os
import sys
import glob
import time
import optparse
import subprocess
from datetime import datetime
# Lib from avclass project
path = os.path.dirname(os.path.abspath(__file__))
libpath = os.path.join(path, 'lib/')
sys.path.insert(0, libpath)
import evaluate_clustering as ec

# Global vars
# Working path, here we'll put the generated files
working_path = os.getcwd()
fishdbc_path = os.path.join(path, 'fishdbc_notebook/')
parse_clusters_path = os.path.join(path, 'parse_results/')
features_path = os.path.join(fishdbc_path, 'features/')

runid = 0

filters = {
            # Cluster no empty, no background, benign conns
            'benign_no-empty_no-bg': '',
            # Cluster empty, background, benign conns (all)
            'benign_empty_bg': '-e -B',
            # Cluster no empty, no background, no benign conns (clean)
            'no-benign_no-empty_no-bg': '-b ../data/tranco/tranco_83LV.csv',
            # Cluster empty, no background, no benign conns
            'no-benign_empty_bg': '-e -b ../data/tranco/tranco_83LV.csv',
        }
group_weights = {
            'num-cat weights': '-w',
            'num-cat no weights': '',
            'cli-ser-cert weights': '-g -w',
            'cli-ser-cert no weights': '-g',
        }
# Parameters for running fishdbc
feature_set = {
            'all': '-f {}all.csv'.format(features_path),
            'no timings': '-f {}all_no-timings.csv'.format(features_path),
#            'numeric only': '-f {}numeric_only.csv'.format(features_path),
            'payload only': '-f {}numeric_payload.csv'.format(features_path),
            'no payload': '-f {}no_payload.csv'.format(features_path),
        }
distance = {
            'cosine': '-n -t -d cosine',
            'not norm': '-t -d cosine',
            'no tfidf': '-n -d cosine',
            'not norm not tfidf': '-d cosine',
            'euclidean': '-n -t -d euclidean',
            'not norm euclidean': '-t -d euclidean',
            'not tfidf euclidean': '-n -d euclidean',
            'not norm not tfidf euclidean': '-d euclidean',
        }
hyper_params = {
            'mcs': [5, 4, 3],
            'm': [10, 5, 2],
            'mpts': [10, 5, 4, 3, 2],
        }

def execute_script(spath='', name='', params='', of='', dbg=False):
    try:
        exec_this = '{}{} {}'.format(spath, name, params)
        exec_this += ' > {}'.format(of) if of else ''
        retcode = subprocess.call(exec_this, shell=True)
        if dbg:
            print('Calling {}: {}'.format(name, exec_this))
            msg = 'was terminated by signal' if retcode < 0 else 'returned'
            r = -retcode if retcode < 0 else retcode
            print('{} {}'.format(name, msg), r, file=sys.stderr)
    except OSError as e:
        print('Execution of {} failed:'.format(name), e, file=sys.stderr)

def parse_results(parse_files, ds_path, lb_path, wl_path, move_path, debug):
    this_dir = os.getcwd()
    # Change dir to parse_clustering_results
    os.chdir(parse_clusters_path)

    if not os.path.isdir(move_path):
        if debug:
            print('Creating folder for the parsed results: ', move_path)
        os.makedirs(move_path)

    for c_path in parse_files:
        if debug:
            print('Executing parse_clustering_results.py for ', c_path)
        sname = 'parse_clustering_results.py'
        prms = '--feat {} --clust {} --avc {}'
        if os.path.isfile(wl_path):
            prms += ' --weblog {}'.format(wl_path)
        prms = prms.format(ds_path, c_path, lb_path)
        execute_script(parse_clusters_path, sname, prms, '', debug)
        # Move resulting files
        fdetails = os.path.basename(c_path).replace('.json', '_details.tsv')
        fsummary = os.path.basename(c_path).replace('.json', '_summary.tsv')
        subprocess.run(['mv', fdetails, move_path])
        subprocess.run(['mv', fsummary, move_path])

    # Return to the last dir
    os.chdir(this_dir)

def execute_clustering(dataset, f_p, gw_p, fs_p, d_p, hp_p, th_p, debug=False):
    global runid
    runid += 1
    idx = 0
    base_path = os.path.join(path, 'data/groundtruth/{}'.format(dataset))
    ds_path = '{}.tsv'.format(base_path)
    lb_path = '{}.labels'.format(base_path)
    wl_path = '{}.weblog'.format(base_path)
    gt_path = '{}_truth.tsv'.format(base_path)
    sn_path = 'tls_clustering_grouping_sni.tsv'
    ce_path = 'tls_clustering_grouping_cert.tsv'
    fp_path = 'tls_clustering_grouping_cfp.tsv'
    cn_path = 'tls_clustering_grouping_cnt.tsv'
    ms_path = 'tls_clustering_meanshift_results.csv'
    cluster_types = ['flat', 'hier']
    dbg = '-D' if debug else ''

    # Execute clustering inside fishdbc path
    os.chdir(fishdbc_path)

    ##### Step 1: Pre-clustering, only in the first run

    if runid == 1:
        # Grouping by SNI (ESLD)
        time_start_step1 = time.time()
        sname = 'sni_grouping.py'
        sofile = 'tls_clustering_grouping_sni.out'
        t_p = '-t' if options.tor else ''
        print('==============================')
        print('Grouping vectors by SNI (ESLD)')
        print('Parameters for {}:'.format(sname))
        print('> Dataset: {}'.format(ds_path))
        print('> Filters: {}'.format(f_p))
        print('> Grouping Tor: {}'.format(t_p))
        print('==============================')
        prms = '{} {} {} {} {}'.format(t_p, f_p, dbg, ds_path, '-v 2000000')
        execute_script(fishdbc_path, sname, prms, sofile, debug)
        time_end_step1 = time.time()
        elapsed_step1 = time_end_step1 - time_start_step1
        print('Time elapsed in step 1: {}'.format(elapsed_step1))
        # Scoring for SNI grouping
        if os.path.isfile(sn_path):
            msg = 'SNI GROUPING DATASET: {}'.format(sn_path)
            pp, rp, fp = calculate_scores(gt_path, sn_path, msg)

        # Grouping by Server Certificate using strict comparison (der_hash)
        time_start_step2 = time.time()
        sname = 'cert_grouping.py'
        sofile = 'tls_clustering_grouping_cert.out'
        ce_path = 'tls_clustering_grouping_cert.tsv'
        p_a = '-a' if options.cert_aggressive else ''
        print('==============================')
        print('Grouping vectors by Server Certificate')
        print('Parameters for {}:'.format(sname))
        print('> Aggressive: {}'.format(p_a))
        print('> Dataset: {}'.format(ds_path))
        print('> SNI groups: {}'.format(sn_path))
        print('==============================')
        prms = '{} {} --groups={} {}'.format(dbg, p_a, sn_path, ds_path)
        execute_script(fishdbc_path, sname, prms, sofile, debug)
        time_end_step2 = time.time()
        elapsed_step2 = time_end_step2 - time_start_step2
        print('Time elapsed in step 2: {}'.format(elapsed_step2))
        # Scoring for CERT grouping
        if os.path.isfile(ce_path):
            msg = 'CERT GROUPING DATASET: {}'.format(ce_path)
            pp, rp, fp = calculate_scores(gt_path, ce_path, msg)

        # Grouping by Client Fingerprint
        time_start_step3 = time.time()
        sname = 'fp_grouping.py'
        sofile = 'tls_clustering_grouping_cfp.out'
        print('==============================')
        print('Grouping by Client Fingerprint')
        print('Parameters for {}:'.format(sname))
        print('> Dataset: {}'.format(ds_path))
        print('> CERT groups: {}'.format(ce_path))
        print('==============================')
        prms = '{} --groups={} {}'.format(dbg, ce_path, ds_path)
        execute_script(fishdbc_path, sname, prms, sofile, debug)
        time_end_step3 = time.time()
        elapsed_step3 = time_end_step3 - time_start_step3
        print('Time elapsed in step 3: {}'.format(elapsed_step3))
        # Scoring for FINGERPRINT grouping
        if os.path.isfile(fp_path):
            msg = 'FINGERPRINT GROUPING DATASET: {}'.format(fp_path)
            pp, rp, fp = calculate_scores(gt_path, fp_path, msg)

        # Grouping by Content
        time_start_step4 = time.time()
        sname = 'cnt_grouping.py'
        feat_cnt = feature_set['payload only']
        sofile = 'tls_clustering_grouping_cnt.out'
        p_a = '-a' if options.payload_aggressive else ''
        print('==============================')
        print('Grouping by Content')
        print('Parameters for {}:'.format(sname))
        print('> Aggressive: {}'.format(p_a))
        print('> Dataset: {}'.format(ds_path))
        print('> CFP groups: {}'.format(fp_path))
        print('> Features: {}'.format(feat_cnt))
        print('==============================')
        prms = '{} {} -g {} {} {}'.format(dbg, p_a, fp_path, feat_cnt, ds_path)
        execute_script(fishdbc_path, sname, prms, sofile, debug)
        time_end_step4 = time.time()
        elapsed_step4 = time_end_step4 - time_start_step4
        print('Time elapsed in step 4: {}'.format(elapsed_step4))
        # Scoring for CONTENT grouping
        if os.path.isfile(cn_path):
            msg = 'CONTENT GROUPING DATASET: {}'.format(cn_path)
            pp, rp, fp = calculate_scores(gt_path, cn_path, msg)

        # Parse results
        move_path = os.path.join(working_path, dataset, 'parser/groups/')
        grouped_by = ['sni', 'cert', 'cfp', 'cnt']
        gfilename = 'tls_clustering_grouping_{}.json'
        gfilename = os.path.join(fishdbc_path, gfilename)
        parse_files = [gfilename.format(e) for e in grouped_by]
        parse_results(parse_files, ds_path, lb_path, wl_path, move_path, debug)

        if options.meanshift:
            # Using MeanShift to generate the groups
            time_start_meanshift = time.time()
            sname = 'apply_meanshift.py'
            sofile = 'tls_clustering_meanshift.out'
            feat_cnt = feature_set['payload only']
            bw = '-w 0.1 -n'
            print('==============================')
            print('Clustering using MeanShift')
            print('Parameters for {}:'.format(sname))
            print('> Dataset: {}'.format(ds_path))
            print('> Filters: {}'.format(f_p))
            print('> Features: {}'.format(feat_cnt))
            print('> Bandwidth: {}'.format(bw))
            print('==============================')
            o_p = '-v 2000000 -s {}'.format(dbg)
            prms = '{} {} {} {} {}'.format(bw, f_p, feat_cnt, o_p, ds_path)
            execute_script(fishdbc_path, sname, prms, sofile, debug)
            time_end_meanshift = time.time()
            elapsed_meanshift = time_end_meanshift - time_start_meanshift
            print('Time elapsed in MeanShift: {}'.format(elapsed_meanshift))
            # Scoring for MEANSHIFT clustering
            if os.path.isfile(ms_path):
                msg = 'MEANSHIFT DATASET: {}'.format(ms_path)
                pp, rp, fp = calculate_scores(gt_path, ms_path, msg)

            # Grouping by Contents using MeanShift
            time_start_step5 = time.time()
            sname = 'merge_clusters.py'
            sofile = 'tls_clustering_grouping_meanshift.out'
            out_path = 'tls_clustering_grouping_meanshift'
            mg_path = '{}.tsv'.format(out_path)
            print('==============================')
            print('Grouping by Contents using MeanShift clustering')
            print('Parameters for {}:'.format(sname))
            print('> Dataset: {}'.format(ds_path))
            print('> Groups: {}'.format(fp_path))
            print('> Clusters: {}'.format(ms_path))
            print('> Output file: {}'.format(out_path))
            print('==============================')
            prms = '--groups={} --clusters={}'.format(fp_path, ms_path)
            prms += ' {} -o {} {}'.format(dbg, out_path, ds_path)
            execute_script(fishdbc_path, sname, prms, sofile, debug)
            time_end_step5 = time.time()
            elapsed_step5 = time_end_step5 - time_start_step5
            print('Time elapsed in step 5: {}'.format(elapsed_step5))
            # Scoring for CONTENT grouping using MEANSHIFT
            if os.path.isfile(mg_path):
                msg = 'CONTENT-BY-MEANSHIFT GROUPING DATASET: ' + mg_path
                calculate_scores(gt_path, mg_path, msg)

            # Parse results and move to the same dir than grouping
            gfilename = 'tls_clustering_meanshift.json'
            parse_files = [os.path.join(fishdbc_path, gfilename)]
            gfilename = 'tls_clustering_grouping_meanshift.json'
            parse_files.append(os.path.join(fishdbc_path, gfilename))
            parse_results(parse_files, ds_path, lb_path,
                    wl_path, move_path, debug)

    ##### Step 3: Clustering

    if not (options.single or options.score):
        return 0, 0, 0

    time_start_clust = time.time()
    print('============================')
    print('Run ID: {}'.format(runid))
    sname = 'apply_fishdbc.py'
    sofile = 'tls_all_clustering.out'
    if debug:
        print('Parameters for {}'.format(sname))
        print('> Dataset: {}'.format(ds_path))
        print('> Filters: {}'.format(f_p))
        print('> Features: {}'.format(fs_p))
        print('> Group-Weights: {}'.format(gw_p))
        print('> Distance: {}'.format(d_p))
        print('> Hyper-params: {}'.format(hp_p))
        print('> Threshold: {}'.format(th_p))
    print('============================')

    # Cluster maximum 2 million connections, yielding singleton clusters
    o_p = '-v 2000000 -s {}'.format(dbg)
    prms = '{} {} {} {}'.format(f_p, fs_p, gw_p, d_p)
    prms += ' {} {} {} {}'.format(hp_p, th_p, o_p, ds_path)
    execute_script(fishdbc_path, sname, prms, sofile, debug)
    time_end_clust = time.time()
    elapsed_clust = time_end_clust - time_start_clust
    print('Time elapsed in clustering: {}'.format(elapsed_clust))

    # Continue if the output files were created
    cl_path = os.path.join(fishdbc_path, 'tls_all_flat_results.csv')
    if os.path.isfile(cl_path):

        ############ Merge groups by content
        # Grouping by Contents
        time_start_step6 = time.time()
        sname = 'merge_clusters.py'
        sofile = 'tls_all_clustering_grouping_pay.out'
        ct_path = 'tls_all_clustering_grouping_pay.tsv'
        print('==============================')
        print('Grouping by Contents using clustering')
        print('Parameters for {}:'.format(sname))
        print('> Dataset: {}'.format(ds_path))
        print('> Groups: {}'.format(fp_path))
        print('> Clusters: {}'.format(cl_path))
        print('==============================')
        prms = '--groups={} --clusters={}'.format(fp_path, cl_path)
        prms += ' {} {}'.format(dbg, ds_path)
        execute_script(fishdbc_path, sname, prms, sofile, debug)
        time_end_step6 = time.time()
        elapsed_step6 = time_end_step6 - time_start_step6
        print('Time elapsed in step 6: {}'.format(elapsed_step6))

        # Scoring for CONTENT grouping
        if os.path.isfile(ct_path):
            msg = 'CONTENT-BY-CLUSTERS GROUPING DATASET: {}'.format(ct_path)
            msg += ', RUN ID: {}'.format(runid)
            pp, rp, fp = calculate_scores(gt_path, ct_path, msg)
        ####################################

        # Create a folder for the output files
        r = '{}/fishdbc/{}/'.format(dataset, runid)
        results_path = os.path.join(working_path, r)
        if debug:
            print('Folder to save raw results: {}'.format(results_path))
        os.makedirs(results_path)
        # Move files generated in each run (tls_all_*)
        for f in glob.glob(fishdbc_path + "tls_all_*", recursive=False):
            subprocess.run(['mv', f, results_path])

        ##### Step 4: Processing of clustering results

        # Execute parse_clustering_results.py on flat and hier clusters
        for ctype in cluster_types:
            cfile = 'tls_all_{}_clustering_fishdbc.json'.format(ctype)
            cfile = os.path.join(results_path, cfile)
            parse_files = [cfile]
            if ctype == 'flat':
                gfile = 'tls_all_clustering_grouping_pay.json'
                gfile = os.path.join(results_path, gfile)
                parse_files.append(gfile)
            # Dir for the output files
            rc = '{}/parser/{}/{}/'.format(dataset, runid, ctype)
            move_path = os.path.join(working_path, rc)
            parse_results(parse_files, ds_path, lb_path,
                    wl_path, move_path, debug)

            if debug:
                print('> Completed RUNID: {}, DATASET: {}, CLUSTER: {}'\
                        .format(runid, dataset, ctype))

        ##### Step 5: Calculate the scores

        msg = 'RUNID: {}, DATASET: {}.'.format(runid, dataset)
        rs_path = os.path.join(results_path, 'tls_all_flat_results.csv')
        return calculate_scores(gt_path, rs_path, msg)

    else:
        # Change dir to working
        os.chdir(working_path)

        msg = 'Results file not found for RUNID:{}, DATASET:{}. '
        msg += 'Skipping scoring.'
        print(msg.format(runid, dataset))
        return 0, 0, 0

def calculate_scores(gt_path, r_path, idmsg):
    if not os.path.isfile(gt_path):
        print('Groundtruth file not found. Skipping scoring.')
        print(idmsg)
        return 0, 0, 0
    # Read the groundtruth
    truth_dict = {}
    idx_conn_uid = 0
    with open(gt_path) as f:
        for pos, line in enumerate(f):
            if not pos:
                headers = line.strip('\n').split('\t')
                for idxh in range(len(headers)):
                    if headers[idxh] == 'conn_uid':
                        idx_conn_uid = idxh
                        break
                continue
            values = line.strip('\n').split('\t')
            # Concatenating sha2 and conn_uid
            key = '{}:{}'.format(values[1], values[idx_conn_uid])
            truth_dict.update({key : values[0]})

    # Read the estimated results to create a dict for the scoring
    guess_dict = {}
    ids = set()
    with open(r_path) as f:
        for line in f:
            values = line.strip('\n').split(',')
            # The results file only has sha2:conn_uid,label
            guess_dict.update({values[0] : values[1]})
            ids.add(values[0])

    p, r, f = ec.eval_precision_recall_fmeasure(truth_dict, guess_dict, ids)
    print("------ Scores ------")
    print(idmsg)
    print("--------------------")
    print("Precision: %s%%" % p)
    print("Recall: %s%%" % r)
    print("F-Measure: %s%%" % f)
    print("--------------------")
    return p, r, f

def best_score(p, r, f, p_max, r_max, f_max, score='f1'):
    if score == 'f1':
        if f > f_max or (f == f_max and p > p_max):
            return p, r, f, True
    elif score == 'precision':
        if p > p_max or (p == p_max and r > r_max):
            return p, r, f, True
    elif score == 'recall':
        if r > r_max or (r == r_max and p > p_max):
            return p, r, f, True
    return p_max, r_max, f_max, False

def main(options, args):
    # For now these are hardcoded per run
#    filter = filters['benign_empty_bg']
    filter = filters['benign_no-empty_no-bg']
    thld = '-T 0'

    if options.single:
        print('Single execution')
        h_p = '--mpts=2 --mcs=3 --m=5'
        execute_clustering(args[0], filter,
#                group_weights['num-cat weights'],
                group_weights['cli-ser-cert no weights'],
#                feature_set['no payload'], distance['cosine'], h_p, thld,
                feature_set['payload only'], distance['cosine'], h_p, thld,
                options.debug)
        return

    if not options.score:
        print('Grouping execution')
        execute_clustering(args[0], filter, '', '', '', '', '', options.debug)
        return

    best_f = best_d = best_h = best_gw = ''
    p_max = r_max = f_max = 0
    best_run = 0
    h_p = '--mpts=2 --mcs=3 --m=5'
    print('Maximizing score using: {}'.format(options.score))
    print('----------------------------------------')
    print('EXPERIMENT 0')
    for gw_k, gw_v in group_weights.items():
        p, r, f = execute_clustering(args[0], filter, gw_v, feature_set['all'],
                distance['cosine'], h_p, thld, options.debug)
        p_max, r_max, f_max, updated = \
                best_score(p, r, f, p_max, r_max, f_max, options.score)
        best_gw = gw_k if updated else best_gw
        best_run = runid if updated else best_run
    print('----------------------------------------')
    print('Best results for EXP0: {}'.format(best_gw))
    print('RUN ID: {}'.format(best_run))
    print('Precision: {}%'.format(p_max))
    print('Recall: {}%'.format(r_max))
    print('F-Measure: {}%'.format(f_max))
    print('----------------------------------------')
    # Use the default value if there are no scores
    best_gw = 'num-cat weights' if best_gw == '' else best_gw
    p_max = r_max = f_max = 0
    print('----------------------------------------')
    print('EXPERIMENT 1')
    for fs_k, fs_v in feature_set.items():
        p, r, f = execute_clustering(args[0], filter, group_weights[best_gw],
                 fs_v, distance['cosine'], h_p, thld, options.debug)
        p_max, r_max, f_max, updated = \
                best_score(p, r, f, p_max, r_max, f_max, options.score)
        best_f = fs_k if updated else best_f
        best_run = runid if updated else best_run
    print('----------------------------------------')
    print('Best results for EXP1: {}'.format(best_f))
    print('RUN ID: {}'.format(best_run))
    print('Precision: {}%'.format(p_max))
    print('Recall: {}%'.format(r_max))
    print('F-Measure: {}%'.format(f_max))
    print('----------------------------------------')
    # Use the default value if there are no scores
    best_f = 'all' if best_f == '' else best_f
    # TODO temporary fix to force payload only features
#    best_f = 'payload only'
    p_max = r_max = f_max = 0
    print('----------------------------------------')
    print('EXPERIMENT 2')
    for d_k, d_v in distance.items():
        p, r, f = execute_clustering(args[0], filter, group_weights[best_gw],
                feature_set[best_f], d_v, h_p, thld, options.debug)
        p_max, r_max, f_max, updated = \
                best_score(p, r, f, p_max, r_max, f_max, options.score)
        best_d = d_k if updated else best_d
        best_run = runid if updated else best_run
    print('----------------------------------------')
    print('Best results for EXP2: {}'.format(best_d))
    print('RUN ID: {}'.format(best_run))
    print('Precision: {}%'.format(p_max))
    print('Recall: {}%'.format(r_max))
    print('F-Measure: {}%'.format(f_max))
    print('----------------------------------------')
    # Use the default value if there are no scores
    best_d = 'cosine' if best_d == '' else best_d
    p_max = r_max = f_max = 0
    print('----------------------------------------')
    print('EXPERIMENT 3')
    for mpts in hyper_params['mpts']:
        for mcs in hyper_params['mcs']:
            for m in hyper_params['m']:
                hp_params = '--mpts={} --mcs={} --m={}'.format(mpts, mcs, m)
                p, r, f = execute_clustering(args[0], filter,
                        group_weights[best_gw], feature_set[best_f],
                        distance[best_d], hp_params, thld, options.debug)
                p_max, r_max, f_max, updated = \
                        best_score(p, r, f, p_max, r_max, f_max, options.score)
                best_h = hp_params if updated else best_h
                best_run = runid if updated else best_run
    print('----------------------------------------')
    print('Best results for EXP3: {}'.format(best_h))
    print('RUN ID: {}'.format(best_run))
    print('Precision: {}%'.format(p_max))
    print('Recall: {}%'.format(r_max))
    print('F-Measure: {}%'.format(f_max))
    print('----------------------------------------')
    return

if __name__ == '__main__':
    time_start = time.time()
    print('Starting at {}'.format(datetime.now()))

    parser = optparse.OptionParser(
            usage="\n\t%prog [options] dataset_name" + \
                    "\n\n\tdataset_name: A valid dataset_name must be" +\
                    " provided. This implies that a dataset file (in .tsv" +\
                    " format) and an AVClass file (with .labels extension)" +\
                    " using the same name exist in data/groundtruth/. If" +\
                    " an optional [dataset_name]_truth.tsv file is present," +\
                    " this script will use it as a groundtruth in order to" +\
                    " calculate the precision, recall and F1 scores. If a" +\
                    " [dataset_name].weblog file exists, it will be used" +\
                    " to associate each connection with its requested URIs" +\
                    " as they appear in such log.",
            version="%prog 1.0")
    parser.add_option(
            '-s', '--score', action='store', dest='score',
            type='choice', choices=['f1', 'precision', 'recall'],
            help='Choose score to maximize for FISHDBC: [f1|precision|recall]')
#            default='precision')
    parser.add_option(
            '-S', '--single', action='store_true', dest='single',
            help='Execute FISHDBC clustering once using fixed parameters',
            default=False)
    parser.add_option(
            '-m', '--meanshift', action='store_true', dest='meanshift',
            help='Execute MeanShift clustering',
            default=False)
    parser.add_option(
            '-c', '--cert-aggresive', action='store_true',
            dest='cert_aggressive', default=False,
            help='Generate certificate groups using aggressive merging')
    parser.add_option(
            '-p', '--payload-aggressive', action='store_true',
            dest='payload_aggressive', default=False,
            help='Generate payload groups using aggressive merging')
    parser.add_option(
            '-t', '--tor', action='store_true',
            dest='tor', default=False,
            help='Collect tor-like connections in the SNI grouping')
    parser.add_option(
            '-d', '--debug', action='store_true', dest='debug',
            help='Print debug messages', default=False)

    options, args = parser.parse_args()

    if len(args) == 1:
        fpath = '{}/data/groundtruth/{}'.format(path, args[0])
        ds_path = '{}.tsv'.format(fpath)
        lb_path = '{}.labels'.format(fpath)
        if not os.path.isfile(ds_path) or not os.path.isfile(lb_path):
            parser.error("Dataset or labels file not found. Aborting...")
    else:
        parser.error("Please specify a valid dataset name. Exiting.")

    main(options, args)

    # Move the fishdbc group files before exiting
    r = os.path.join(args[0], 'fishdbc/groups/')
    results_path = os.path.join(working_path, r)
    if options.debug:
        print('Saving raw group files to: {}'.format(results_path))
    os.makedirs(results_path)
    # Move files
    groupsf = os.path.join(fishdbc_path, "tls_clustering_grouping_*")
    for f in glob.glob(groupsf, recursive=False):
        subprocess.run(['mv', f, results_path])
    groupsf = os.path.join(fishdbc_path, "tls_clustering_meanshift*")
    for f in glob.glob(groupsf, recursive=False):
        subprocess.run(['mv', f, results_path])

    time_end = time.time()
    print('Done at {}'.format(datetime.now()))
    print('Total time elapsed: {}'.format(time_end - time_start))
