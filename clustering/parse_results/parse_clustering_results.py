#!/usr/bin/env python
# -*- coding: utf-8 -*-
import argparse
import json
import sys
import csv
import os
import operator
import hashlib
from collections import Counter, OrderedDict, defaultdict
#from publicsuffixlist import PublicSuffixList
#from publicsuffix import fetch as pfetch
from datetime import datetime
sys.path.insert(1, '../lib/')
#import extract_tld_esld
import grouping

# Global variables
#psl = object()
#tranco_list = set()
# CLUST_RES_FILE = '../data/clustering/tls_all-flat.json'
# ALEXA FILE
ALEXA_FILE = '../data/alexa/alexa_top10K.csv'
# TRANCO FILE
TRANCO_FILE = '../data/tranco/tranco_83LV.csv'
#TRANCO_FILE = '../data/tranco/tranco_LJZ4.csv'
# IANA Service name and transport protocol port number registry
# Reference: http://www.iana.org/
# go/rfc6335
# assignments/service-names-port-numbers/service-names-port-numbers.xhtml
KNOWN_PORTS = '../data/iana/service-names-port-numbers.csv'

#OSUM_FILE = 'clusters_summary.tsv'
#ODET_FILE = 'clusters_details.tsv'

def pprint_dict(dic):
    '''Pretty prints a dictionary'''
    import pprint
    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(dic)

#def is_in_tranco(server_name=''):
#    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
#    return e2ld in tranco_list

def main(args):
    """ """
#    global psl
#    global tranco_list

    # Fetch PublicSuffix list and load it
#    psl_file = pfetch()
#    psl = PublicSuffixList(psl_file)
    grouping.load_psl()
    grouping.load_tranco_list(TRANCO_FILE)

    # Load Alexa list
    alexa = {}
    with open(ALEXA_FILE, 'r') as fr:
        for pos, line in enumerate(fr):
            alexa[line.strip('\n').lower()] = None

    # Load Tranco list
#    with open(TRANCO_FILE, 'r') as tf:
#        for line in tf:
#            domain_name = line.split(',')[1]
#            tranco_list.add(domain_name.strip())

    # Load the list of known ports, if present
    known_ports = {}
    if os.path.isfile(KNOWN_PORTS):
        with open(KNOWN_PORTS, 'r') as kp:
            csv_dict = csv.DictReader(kp)
            for row in csv_dict:
                portn, proto = row['Port Number'], row['Transport Protocol']
                if not portn or not proto:
                    continue
                pkey = '{}/{}'.format(portn, proto)
                p = {
                        'service': row['Service Name'],
                        'desc': row['Description'],
                        'unauth': row['Unauthorized Use Reported'],
                        'notes': row['Assignment Notes'],
                        'others': {},
                    }
                if pkey in known_ports:
                    last_p = known_ports[pkey]
                    others = {p['service']: p['desc']}
                    last_p['others'].update(others)
                    known_ports.update(last_p)
                else:
                    known_ports.update({pkey: p})

    # Parse AVClass results
    avc_fams = {}
    if args.avc:
        with open(args.avc, 'r') as fr:
            for pos, line in enumerate(fr):
                fhash, fam, pup = line.strip('\n').split('\t')
                if 'SINGLETON' in fam:
                    continue
                avc_fams[fhash] = fam

    # Parse Weblog file if it is present
    weblog = OrderedDict()
    idx_id = 0
    idx_ts = 0
    idx_c_ip = 0
    idx_r_ip = 0
    idx_c_port = 0
    idx_r_port = 0
    idx_cs_uri = 0
    if args.weblog:
        with open(args.weblog, 'r') as wl:
            for pos, line in enumerate(wl):
                if not pos:
                    wheaders = line.strip('\n').split('\t')
                    wheaders[0].replace('#Fields:   ', '')
                    #TODO Check for different field names in other weblog files
                    for idx in range(len(wheaders)):
                        if wheaders[idx] == 'id':
                            idx_id = idx
                        elif wheaders[idx] == 'timestamp':
                            idx_ts = idx
                        elif wheaders[idx] == 'c-ip':
                            idx_c_ip = idx
                        elif wheaders[idx] == 'c-port':
                            idx_c_port = idx
                        elif wheaders[idx] == 'r-ip':
                            idx_r_ip = idx
                        elif wheaders[idx] == 'r-port':
                            idx_r_port = idx
                        elif wheaders[idx] == 'cs-uri':
                            idx_cs_uri = idx
                    continue
                wlfields = line.strip('\n').split('\t')
                #Calculate md5_conn_id
                #md5_str = '{},{},{},{},{},{}'.format(
                md5_str = '{},{},{},{},{}'.format(
                        wlfields[idx_c_ip], wlfields[idx_c_port],
                        wlfields[idx_r_ip], wlfields[idx_r_port],
                        'tcp').encode('utf-8')
                        # At the end we ignore timestamp as it is wrong taken
                        #'tcp', wlfields[idx_ts])
                m = hashlib.md5()
                m.update(md5_str)
                md5_conn_id = m.hexdigest()
                # Saving it using md5/id to prevent collitions
                # TODO Be sure these are time-ordered in weblog file
                m_id = '{}/{}'.format(md5_conn_id, wlfields[idx_id])
                if m_id in weblog.keys():
                    uris = '{}\n{}'.format(weblog[m_id], wlfields[idx_cs_uri])
                    weblog[m_id] = uris
                else:
                    weblog[m_id] = wlfields[idx_cs_uri]

    # Get SNIs and other info from feature vectors
    conn_snis = {}
    conn_dst_ips = {}
    resumed = {}
    publishers = {}
    tls_fingerprints = {}
    sequences = {}
    dst_ports = {}
    md5_conn_ids = {}
    ts_min, ts_max = 0, 0
    # TODO Remeber to optimize this, as not every conn has been clustered
    with open(args.feat, 'r') as fr:
        csv_dict = csv.DictReader(fr, delimiter='\t')
        for elem in csv_dict:
            sha2_conn = '{}:{}'.format(elem['sha2'], elem['conn_uid'])
            conn_snis[sha2_conn] = elem['server_name']
            conn_dst_ips[sha2_conn] = elem['s_dst_ip']
            seq = ','.join([elem['msg_size_{}_{}'.format(d, p)] for d, p in
                zip(('cscscs'), ('001122'))])
            sequences[sha2_conn] = seq
            dst_ports[sha2_conn] = elem['s_dst_port']
            if args.weblog:
                md5_conn_ids[sha2_conn] = elem['md5_conn_id']
            if elem['resumed'] == 'True':
                resumed[sha2_conn] = None
            if elem['s_leaf_cert_subject'] != '-':
                publishers[sha2_conn] = elem['s_leaf_cert_subject']
            if elem['c_tls_fingerprint_label'] != '-':
                tls_fingerprints[sha2_conn] = elem['c_tls_fingerprint_label']
            # Check min and max start_time, as '2017-11-02 22:43:10:656656'
            ts = elem['start_time'].strip()
            try:
                dt = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S:%f')
                if not ts_min and not ts_max:
                    ts_min = ts_max = dt
                else:
                    if dt < ts_min:
                        ts_min = dt
                    elif dt > ts_max:
                        ts_max = dt
            except ValueError:
                continue
    # Info for the summary file
    summary = []
    snis_count = {}
    fams_count = {}
    ports_count = {}
    seq_ben = defaultdict(list)
    seq_mal = defaultdict(list)
    sni_nulls = 0
    with open(args.clust, 'r') as fr:
        for pos, line in enumerate(fr):
            res = json.loads(line.strip('\n'))
            for cl in res:
                snis = set()
                for el_id in cl['ids']:
                    sha2, conn_id = el_id.split(':')
                    try:
                        fam = avc_fams[sha2]
                    except KeyError:
                        fam = "None"
                    # Search for the SNIs in this cluster
                    sni = conn_snis[el_id].upper()
                    if sni != 'NULL':
                        snis.add(sni.lower())
                        # Count sequences for malicious/benign domains
                        if grouping.is_in_tranco(sni.lower()):
                            seq_ben[sequences[el_id]].append(sni.lower())
                        else:
                            seq_mal[sequences[el_id]].append(sni.lower())
                    else:
                        sni_nulls += 1
                    # Add the family to the list
                    fgpt = tls_fingerprints.get(el_id, '-')
                    if fam in fams_count:
                        fams_count[fam]['conns'] += 1
                        fams_count[fam]['fgpts'].add(fgpt)
                    else:
                        fams_count[fam] = {
                                'conns': 1,
                                'fgpts': {fgpt}
                            }
                    # Add this s_dst_port to the list of ports
                    p = dst_ports[el_id]
                    ports_count[p] = ports_count[p] + 1 \
                            if p in ports_count else 1
                    row = [sha2, conn_id, str(cl['label']),
                           fam, conn_snis[el_id]]
                    summary.append(row)
                if str(cl['label']) != '-1':
                    snis_count[cl['label']] = snis

    # Number of SNIs in each Cluster
    cl_total = len(snis_count)
    cl_mult_snis = 0
    cl_mult_desc = ''
    s_snis = sorted(snis_count.items(), key=lambda k: len(k[1]), reverse=True)
    for cl_label, snis in s_snis:
        if len(snis) > 1:
            cl_mult_snis += 1
            cl_mult_desc += '#\t Cluster {}: {} SNIs\n#\t\t {}\n'\
                    .format(cl_label, len(snis), snis)
    cl_perc = cl_mult_snis*100/cl_total if cl_total else 0

    # Number of families in the dataset
    f_total = len(fams_count)
    fg_total = len(set(tls_fingerprints.values()))
    c_total = sum(fams_count[f]['conns'] for f in fams_count.keys())
    f_desc = ''
    sorted_fams = sorted(fams_count.items(), key=lambda k: k[1]['conns'],
            reverse=True)
    for fam, d in sorted_fams:
        nconn = d['conns']
        fgpts = d['fgpts'].difference({'-'})
        nfpts = len(fgpts)
        cfgperc = nconn*100/c_total if c_total else 0
        f_desc += '\n#\t {} ({:.4f}%)\n'.format(fam, cfgperc)
        f_desc += '#\t\t Conns:\t{}\n'.format(nconn)
        f_desc += '#\t\t Client fingerprints:\t{}\n#\t\t\t'.format(nfpts)
        f_desc += '\n#\t\t\t'.join(fgpt for fgpt in fgpts)

    # Number of fingerprints per family
    sorted_fams_fgp = sorted(fams_count.items(),
            key=lambda k: len(k[1]['fgpts']-{'-'}), reverse=True)
    ff_total = 0
    ff_perc = 0
    ff_desc = ''
    for fam, d in sorted_fams_fgp:
        nfpts = len(d['fgpts'].difference({'-'}))
        if nfpts > 1:
            ff_desc += ' {}: {};'.format(fam, nfpts)
            ff_total += 1
    ff_perc = ff_total*100/f_total if f_total else 0

    # Number of non generic fingerprints per family
    f_nongen = {'-', 'ie;10;;;;', 'microsoft socket;;;;;'}
    sorted_fams_fgp_nongen = sorted(fams_count.items(),
            key=lambda k: len(k[1]['fgpts']-f_nongen), reverse=True)
    fn_total = 0
    fn_perc = 0
    fn_desc = ''
    for fam, d in sorted_fams_fgp_nongen:
        nfpts = len(d['fgpts'].difference(f_nongen))
        if nfpts > 1:
            fn_desc += ' {}: {};'.format(fam, nfpts)
            fn_total += 1
    fn_perc = fn_total*100/f_total if f_total else 0

    # Number of ports in the dataset
    pt_total = len(ports_count)
    cn_total = sum(ports_count.values())
    pt_desc = ''
    sorted_ports = sorted(ports_count.items(), key=lambda k: k[1], reverse=True)
    for k, v in sorted_ports:
        pt_desc += '#\t {}: {} ({:.4f}%)\n'\
                .format(k, v, v*100/cn_total if cn_total else 0)
        # Search it in the well-known ports list (IANA)
        if k in known_ports:
            k_service = known_ports[k]['service']
            k_desc = known_ports[k]['desc']
            k_unauth = known_ports[k]['unauth']
            k_notes = known_ports[k]['notes']
            k_others = known_ports[k]['others']
            if k_service:
                pt_desc += '#\t\t {}'.format(k_service)
            pt_desc += ': {}\n'.format(k_desc) if k_desc else '\n'
            if k_notes:
                pt_desc += '#\t\t IANA notes: {}\n'.format(k_notes)
            if k_unauth:
                pt_desc += '#\t\t *{}\n'.format(k_unauth)
            if k_others:
                pt_desc += '#\t\t Other known service-names:\n'
                for e, v in k_others.items():
                    pt_desc += '#\t\t\t {}: {}\n'.format(e, v)
        else:
            pt_desc += '#\t\t Not defined by IANA\n'

    # Number of sequences and distribution per SNIs
    seqs = {'total': len(set(seq_ben).union(set(seq_mal)))}
    seqs.update({'inter': set(seq_ben).intersection(set(seq_mal))})
    seqs.update({'it': len(seqs['inter'])})
    seqs.update({'ip': '{:.2f}'.format(seqs['it']*100/seqs['total'])})
    sbi = [seq_ben[k] for k in set(seq_ben)-seqs['inter']]
    smi = [seq_mal[k] for k in set(seq_mal)-seqs['inter']]
    seqs['bc'] = [x for l in sbi for x in l]
    seqs['mc'] = [x for l in smi for x in l]
    seqs.update({'bt': len(seq_ben) - len(seqs['inter'])})
    seqs.update({'mt': len(seq_mal) - len(seqs['inter'])})
    totseq = seqs['bt']+seqs['mt']
    seqs.update({'btp': '{:.2f}'.format(seqs['bt']*100/totseq)})
    seqs.update({'mtp': '{:.2f}'.format(seqs['mt']*100/totseq)})
    seqs.update({'bct': len(seqs['bc']), 'mct': len(seqs['mc'])})
    totconns = seqs['bct']+seqs['mct']
    seqs.update({'bctp': '{:.2f}'.format(seqs['bct']*100/(totconns))})
    seqs.update({'mctp': '{:.2f}'.format(seqs['mct']*100/(totconns))})
    seqs.update({'bd': set(seqs['bc']), 'bdt': len(set(seqs['bc']))})
    seqs.update({'md': set(seqs['mc']), 'mdt': len(set(seqs['mc']))})
    totsnis = seqs['bdt']+seqs['mdt']
    seqs.update({'bdtp': '{:.2f}'.format(seqs['bdt']*100/(totsnis))})
    seqs.update({'mdtp': '{:.2f}'.format(seqs['mdt']*100/(totsnis))})
    seqs.update({'brsc': seqs['bt']/seqs['bct'] if seqs['bct'] else 0})
    seqs.update({'brsd': seqs['bt']/seqs['bdt'] if seqs['bdt'] else 0})
    seqs.update({'mrsc': seqs['mt']/seqs['mct'] if seqs['mct'] else 0})
    seqs.update({'mrsd': seqs['mt']/seqs['mdt'] if seqs['mdt'] else 0})

    # Store summary resutlts
    jsonpath, jsonfile = os.path.split(args.clust)
    osum_file = jsonfile.replace('.json', '_summary.tsv')
#    sys.stdout.write('[-] Storing summary results to: {}\n'.format(OSUM_FILE))
    sys.stderr.write('[-] Storing summary results to: {}\n'.format(osum_file))
#    with open(OSUM_FILE, 'w') as fw:
    with open(osum_file, 'w') as fw:
        # Write the general SNI info
        fw.write('# Total clusters:\t{}\n'.format(cl_total))
        fw.write('# Total connections:\t{}\n'.format(cn_total))
        conns_sni = cn_total - sni_nulls
        fw.write('#\t Having SNI:\t{}\n'.format(conns_sni))
        fw.write('# Total families:\t{}\n'.format(f_total))
        fw.write('# Total fingerprints:\t{}\n'.format(fg_total))
        fw.write('# Min timestamp found:\t{}\n'.format(ts_min))
        fw.write('# Max timestamp found:\t{}\n'.format(ts_max))
        # Write sequences distribution
        fw.write('#\n# Different sequences found:\t{}\n'.format(seqs['total']))
        fw.write('#\t Intersection:\t{} ({}%)\n'.format(seqs['it'], seqs['ip']))
        fw.write('#\t\t {}\n'.format(seqs['inter']))
        fw.write('#\t In TRANCO (benign):\n')
        fw.write('#\t\t Sequences:\t{} ({}%)\n'.format(seqs['bt'], seqs['btp']))
        fw.write('#\t\t Conns:\t\t{} ({}%)\n'.format(seqs['bct'], seqs['bctp']))
        fw.write('#\t\t SNIs:\t\t{} ({}%)\n'.format(seqs['bdt'], seqs['bdtp']))
        fw.write('#\t\t\t{}\n'.format(seqs['bd']))
        fw.write('#\t\t Ratio (seq/sni):\t{}\n'.format(seqs['brsd']))
        fw.write('#\t\t Ratio (seq/conn):\t{}\n'.format(seqs['brsc']))
        fw.write('#\t Not in TRANCO (malicious):\n')
        fw.write('#\t\t Sequences:\t{} ({}%)\n'.format(seqs['mt'], seqs['mtp']))
        fw.write('#\t\t Conns:\t\t{} ({}%)\n'.format(seqs['mct'], seqs['mctp']))
        fw.write('#\t\t SNIs:\t\t{} ({}%)\n'.format(seqs['mdt'], seqs['mdtp']))
        fw.write('#\t\t\t{}\n'.format(seqs['md']))
        fw.write('#\t\t Ratio (seq/sni):\t{}\n'.format(seqs['mrsd']))
        fw.write('#\t\t Ratio (seq/conn):\t{}\n'.format(seqs['mrsc']))
        # Write the family distribution
        fw.write('#\n# Families having multiple tls fingerprints: ')
        fw.write('{}; Ratio: {:.4f}% of the total\n'.format(ff_total, ff_perc))
        fw.write('#\t{}\n'.format(ff_desc))
        fw.write('#\n# Families having multiple non generic tls fingerprints: ')
        fw.write('{}; Ratio: {:.4f}% of the total\n'.format(fn_total, fn_perc))
        fw.write('#\t{}\n'.format(fn_desc))
        fw.write('#\n# Family distribution:{}\n#\n'.format(f_desc))
        # Write cluster SNIs
        fw.write('# Clusters having multiple SNIs: {};'.format(cl_mult_snis))
        fw.write(' Ratio: {:.4f}% of the total\n'.format(cl_perc))
        fw.write('{}#\n'.format(cl_mult_desc))
        # Write the general s_dst_port info
        fw.write('# Found usage of {} ports\n'.format(pt_total))
        fw.write('{}#\n'.format(pt_desc))
        # Headers and content
        fw.write('sha2\tconn_id\tcluster_id\tavclass\tsni\n')
        for row in summary:
            fw.write('{}\n'.format('\t'.join(row)))

    odet_file = jsonfile.replace('.json', '_details.tsv')
#    sys.stdout.write('[-] Storing cluster details to: {}\n'.format(ODET_FILE))
    sys.stderr.write('[-] Storing cluster details to: {}\n'.format(odet_file))
    # Storing Cluster details
#    with open(ODET_FILE, 'w') as fw:
    with open(odet_file, 'w') as fw:
        with open(args.clust, 'r') as fr:
            for pos, line in enumerate(fr):
                res = json.loads(line.strip('\n'))
                for pos, cl in enumerate(res):
                    # if pos and pos % 3 == 0:
                    #     exit()
                    # Get number of distinct SNIs
                    snis = set()
                    eslds = defaultdict(list)
                    all_conns_snis = []
                    are_resumed = []
                    pubs = []
                    ips = []
                    tls_fps = []
                    seqs = []
                    for eid in cl['ids']:
                        sni = conn_snis[eid].upper()
                        if sni != 'NULL':
                            snis.add(sni)
                            all_conns_snis.append(sni)
                        if eid in conn_dst_ips:
                            ips.append(conn_dst_ips[eid])
                        if eid in sequences:
                            seqs.append(sequences[eid])
                        # Add publishers
                        try:
                            pubs.append(publishers[eid])
                        except KeyError:
                            pass
                        # Add resumed
                        try:
                            resumed[eid]
                        except KeyError:
                            pass
                        else:
                            are_resumed.append(eid)
                        # Search for tls fingerprint labels
                        try:
                            tls_fps.append(tls_fingerprints[eid])
                        except KeyError:
                            continue
                    # Get number of benign SNIs
                    alexa_snis = []
                    for sni in all_conns_snis:
                        # e2ld = psl.get_public_suffix(sni)
#                        tld, e2ld = extract_tld_esld.get_tld_esld(psl, sni)
                        tld, e2ld = grouping.tld_esld(sni)
                        # print '{},{}'.format(sni.lower(), e2ld.lower())
                        eslds[e2ld.lower()].append(sni.lower())
                        try:
                            alexa[e2ld.lower()]
                        except KeyError:
                            continue
                        else:
                            alexa_snis.append(sni.lower())

                    #fw.write('\nCluster: {}\n'.format(cl['label']))
                    label = '.'.join(map(str, cl['hierarchy'])) \
                            if 'hierarchy' in cl.keys() else cl['label']
                    fw.write('\nCluster: {}\n'.format(label))
                    if 'lambda' in cl.keys():
                        fw.write('\t # Lambda: {}\n'.format(cl['lambda']))
                        epsilon = 1/cl['lambda'] if cl['lambda'] > 0 else 'Inf'
                        fw.write('\t # Epsilon: {}\n'.format(epsilon))
                    fw.write('\t # Conns: {}\n'.format(len(cl['ids'])))
                    fw.write('\t # Resumed Conns: {}\n'.\
                             format(len(are_resumed)))
                    samples = set([i.split(':')[0] for i in cl['ids']])
                    fw.write('\t # Samples: {}\n'.format(len(samples)))
#                    fw.write('\t # SNIs: {}\n'.format(len(snis)))
                    fw.write('\t # Benign connections (SNIs in Alexa): {}\n'.\
                             format(len(alexa_snis)))

#                    fw.write('\t # AVClass Families: {}\n'.\
#                             format(len(cl['avclass'])))

                    # Show all Publishers
                    fw.write('\t # Publishers (#conns:publisher): ')
                    fw.write('{}\n'.format(len(set(pubs))))
                    spubs = sorted(Counter(pubs).items(), key=lambda x: -x[1])
                    for p, c in spubs:
                        fw.write('\t\t {}: {}\n'.format(c, p))

                    # Show all the SNIs
                    fw.write('\t # SNIs (#conns:sni:[benign|malicious]): ')
                    fw.write('{}\n'.format(len(snis)))
                    sorted_snis = sorted(Counter(all_conns_snis).items(),
                                                 key=operator.itemgetter(1),
                                                 reverse=True )
                    for sni, cnt in sorted_snis:
                        s = sni.lower()
                        stype = 'benign' if s in alexa_snis else 'malicious'
                        fw.write('\t\t {}: {}: {}\n'.format(cnt, s, stype))

                    # Show total of different ESLDs
                    fw.write('\t # E2LDs (#conns:e2ld): ')
                    fw.write('{}\n'.format(len(eslds)))
                    s_e2lds = sorted(eslds.items(), key=lambda x: -len(x[1]))
                    for e2ld, csni in s_e2lds:
                        fw.write('\t\t {}: {}\n'.format(len(csni), e2ld))

                    # Show all destination IPs
                    fw.write('\t # Destination IPs: (#conns:IP) ')
                    fw.write('{}\n'.format(len(set(ips))))
                    sips = sorted(Counter(ips).items(), key=lambda x: -x[1])
                    for ip, numips in sips:
                        fw.write('\t\t {}: {}\n'.format(numips, ip))

                    # Show all the AVClass Families
                    fw.write('\t # AVClass Families (#conns:fam): ')
                    fw.write('{}\n'.format(len(cl['avclass'])))
                    for f,c in cl['avclass']:
                        fw.write('\t\t {}: {}\n'.format(c, f))

                    # Show different client tls fingerprints
                    fw.write('\t # TLS Fingerprints (#conns:fingerprint): ')
                    fw.write('{}\n'.format(len(set(tls_fps))))
                    for tlsf, c in Counter(tls_fps).items():
                        fw.write('\t\t {}: {}\n'.format(c, tlsf))

                    # Show different sequences
                    fw.write('\t # Content Sequences (#conns:sequences): ')
                    fw.write('{}\n'.format(len(set(seqs))))
                    sseqs = sorted(Counter(seqs).items(), key=lambda x: -x[1])
                    for cseq, c in sseqs:
                        fw.write('\t\t {}: {}\n'.format(c, cseq))

                    # Show inhomogeneity measure (mean of stdev/avg)
                    num_inh = cl.get('numeric_inhomogeneity', '-')
                    cat_inh = cl.get('categorical_inhomogeneity', '-')
                    fw.write('\t # Inhomogeneity measure:\n')
                    fw.write('\t\t Numeric: {}\n'.format(num_inh))
                    fw.write('\t\t Categorical: {}\n'.format(cat_inh))

                    # Numeric centroids
                    num_centroid = cl.get('numeric_centroid', [])
                    fw.write('\t Numeric Centroid:\n')
                    for en in num_centroid:
                        fw.write('\t\t {}(avg:{}, stdev:{}, stdev/avg:{})\n'.\
                                 format(en['name'], en['avg'],
                                        en['stdev'], en['homs']))

                    # Show the top features
                    cat_centroid = cl.get('top_categorical', [])
                    fw.write('\t Top Categorical Features: \n')
                    for feat in cat_centroid:
                        feat_s = '\t\t {} (mean:{}, stdev:{}, score:{}'
                        feat_s += ', stdev/mean:{})\n'
                        fw.write(feat_s\
                                .format(feat['name'], feat['mean'],
                                        feat['stdev'], feat['score'],
                                        feat['homs']))

                    # Show connection details
                    fw.write('\t Connection Details: \n')
                    dhdr = 'sha2,conn_id,fam,sni,port,publisher,resumed,'
                    dhdr += ','.join(['msg_size_{}_{}'.format(d,p)
                        for d,p in zip(('cscscs'), ('001122'))])
                    fw.write('\t\t {}\n'.format(dhdr))
                    for el_id in cl['ids']:
                        sha2, conn_id = el_id.split(':')
                        try:
                            fam = avc_fams[sha2]
                        except KeyError:
                            fam = "None"
                        try:
                            p = publishers[el_id]
                        except KeyError:
                            p = "NULL"
                        try:
                            resumed[el_id]
                        except KeyError:
                            r = "F"
                        else:
                            r = "T"
                        # Look for the size
                        seq = sequences[el_id] if el_id in sequences else '-'
                        # Look for the dst_port
                        port = dst_ports[el_id] if el_id in dst_ports else ''

                        fw.write('\t\t {},{},{},{},{},{},{},{}\n'.\
                                 format(sha2, conn_id, fam, conn_snis[el_id],
                                        port, p, r, seq))
                    # Show URIs from weblog
                    if args.weblog:
                        fw.write('\t URIs from weblog file: \n')
                        dhdr = 'sha2,conn_id,weblog_id,uri'
                        fw.write('\t\t {}\n'.format(dhdr))
                        id_visited = set()
                        for sha2_id in cl['ids']:
                            c_md5 = md5_conn_ids[sha2_id]
                            sha2, conn_id = sha2_id.split(':')
                            for k, v in weblog.items():
                                key_md5_id = k.split('/')
                                w_md5, w_id = key_md5_id[0], key_md5_id[1]
                                if c_md5 == w_md5:
                                    if w_id in id_visited:
                                        print('Visited md5/id: {}'.format(k))
                                        continue
                                    id_visited.add(w_id)
                                    # Show all URIs having the same id
                                    # TODO: Show rs(Content-Type)
                                    uris = v.split('\n')
                                    for u in uris:
                                        fw.write('\t\t {},{},{},{}\n'\
                                                .format(sha2,conn_id, w_id, u))


if __name__ == '__main__':
    # Process parameters
    argparser = argparse.ArgumentParser(prog='parse_clustering_results',
                      description="Parses clustering results and outputs them"\
                              " in a more user friendly way")
    argparser.add_argument('--feat', help='file with feature vectors',
                           required=True)
    argparser.add_argument('--clust', help='JSON with clustering results',
                           required=True)
    argparser.add_argument('--avc', help='AVClass labels', required=True)
    argparser.add_argument('--weblog', help='Weblog file for URI comparison',
                           required=False)
    args = argparser.parse_args()
    main(args)
