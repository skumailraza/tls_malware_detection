#!/usr/bin/env python
# coding: utf-8

import os
import sys
import csv
import json
import numba
import hdbscan
import flexible_clustering
import numpy as np
import seaborn as sns
import collections
import optparse
from publicsuffixlist import PublicSuffixList
from publicsuffix import fetch as pfetch
from sklearn.datasets.base import Bunch
from matplotlib import pyplot as plt
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics.pairwise import cosine_distances, euclidean_distances
path = os.path.dirname(os.path.abspath(__file__))
libpath = os.path.join(path, '../lib/')
sys.path.insert(0, libpath)
import extract_tld_esld

# Global variables
psl = object()
tranco_list = set()
bg_e2ld_list = set()
bg_fqdn_list = set()

def asjson(clusters, ids, classes, numeric, categorical,
        top_categories=20, lineages=None, hierarchy=None,
        lambdas=None, indexlists={}):
    if lineages is not None:
        iteration = ((lineage, lineage[-1], clusters[lineage[-1]]) \
                for lineage in lineages)
    else:
        if hierarchy is not None:
            iteration = sorted((get_lineage(label, hierarchy), label, c) \
                    for label, c in clusters.items())
        else:
            iteration = ((None, label, c) \
                    for label, c in sorted(clusters.items()))

    maxidx = max(max(c) for c in clusters.values())
    reindexes = {}
    for name, indexes in indexlists.items():
        reindexes[name] = reidx = np.repeat(-1, maxidx + 1)
        reidx[indexes] = np.arange(indexes.size)
    def reindex(c, name):
        reidx = reindexes.get(name)
        if reidx is None:
            return c
        re_c = reidx[c]
        return re_c[re_c != -1]

    res = []
    for clineage, label, c in iteration:
        current = {
            'label': int(label),
            'ids': [ids[i] for i in c],
            'avclass': collections.Counter(classes[i] for i in c).most_common(),
            'size': len(c),
        }
        if clineage is not None:
            current['hierarchy'] = clineage
        if lambdas is not None:
            current['lambda'] = 0 if label == 0 else lambdas[label]
        for name, (features, X, Xz) in numeric:
            avgs = np.nanmean(X[c], 0)
            stdevs = np.nanstd(X[c], 0)
            homs = [stdevs[i]/avgs[i] if avgs[i] > 0 else 0 \
                    for i in range(len(features))]
            std_avgs = np.nanmean(Xz[c], 0)

            re_c = reindex(c, name)
            current[f'{name}_hasvalue'] = len(re_c)
            current[f'{name}_centroid'] = \
                    [{'name': fn, 'avg': avg, 'stdev': std,
                        'std_avg': std_avg, 'homs': homs}
                        for fn, avg, std, std_avg, homs
                        in zip(features, avgs, stdevs, std_avgs, homs)]
            current[f'{name}_inhomogeneity'] = np.sum(homs) / len(features)\
                    if len(features) > 0 else -1
        for name, processed in categorical:
            re_c = reindex(c, name)
            current[f'{name}_hasvalue'] = len(re_c)
            current[f'top_{name}'] = \
                    [{'name': fn, 'score': score, 'mean': mean,
                        'stdev': std, 'homs': homs}
                        for fn, score, mean, std, homs
                        in topfeatures(re_c, processed, top_categories)]
            homs = [current[f'top_{name}'][i]['homs']
                    for i in range(len(current[f'top_{name}']))]
            current[f'{name}_inhomogeneity'] = np.sum(homs) / len(homs)\
                    if len(homs) > 0 else -1
        res.append(current)
    return res

def get_lineage(label, hierarchy):
    res = [label]
    for label in iter(lambda: hierarchy.get(label), None):
        res.append(label)
    return res[::-1]

def get_hierarchy(condensed_tree, skipping_ratio=None, file=sys.stdout):
    stdout = sys.stdout
    sys.stdout = file

    hierarchy = {}
    lambdas = {}
    clusters = collections.defaultdict(list)
    for parent, child, lambda_val, child_size in condensed_tree[::-1]:
        print('parent: {}, child: {}, lambda_val: {}, child_size: {}'.format(parent, child, lambda_val, child_size))
        if child_size == 1:
            clusters[parent].append(child)
        else:
            # assert len(clusters[child]) == child_size
            clusters[parent].extend(clusters[child])
            lambdas[child] = lambda_val
            hierarchy[child] = parent
    clusters = {k: np.array(v) for k, v in clusters.items()}
    print('---- var: clusters ----')
    print(clusters)

    if skipping_ratio is not None:
        nonleaves = set(hierarchy.values())
        skipped = set()
        for child, parent in hierarchy.items():
            if child in nonleaves:
                childsize = clusters[child].size
                parentsize = clusters[parent].size
                if childsize > skipping_ratio * parentsize:
                    del clusters[child]
                    del lambdas[child]
                    skipped.add(child)
        for child, parent in hierarchy.items():
            while parent in skipped:
                parent = hierarchy[parent]
            hierarchy[child] = parent
        for node in skipped:
            print('Skipped: {}'.format(node))
            del hierarchy[node]

    renaming = {c: i for i, c in enumerate(sorted(clusters))}
    print('Renaming: {}'.format(renaming))
    lambdas = {renaming[cid]: l for cid, l in lambdas.items()}
    print('lambdas: {}'.format(lambdas))
    hierarchy = {renaming[c]: renaming[p] for c, p in hierarchy.items()}
    print('hierarchy: {}'.format(hierarchy))
    clusters = {renaming[cid]: c for cid, c in clusters.items()}
    print('clusters: {}'.format(clusters))

    sys.stdout = stdout
    return Bunch(hierarchy=hierarchy, clusters=clusters, lambdas=lambdas)

def get_clusters(labels):
    clusters = collections.defaultdict(list)
    for i, l in enumerate(labels):
        clusters[l].append(i)
    return clusters

def topfeatures(c, processed, nfeat=5):
    featurenames, X, XT = processed
    avgT = XT[c].mean(0).A1
    top = avgT.argsort()[:-nfeat-1:-1]
    Xc = X[c]
    for f in top:
        score = avgT[f]
        if score <= 0:
            break
        Xcf = Xc[:, f].todense()
        xmean = Xcf.mean()
        xstdv = Xcf.std()
        xhoms = xstdv/xmean if xmean > 0 else 0
        yield featurenames[f], score, xmean, xstdv, xhoms

def describer(classes, numeric_features, categorical_features):
    def describe(c, name=None, nclasses=5, nfeat=5, file=sys.stdout):
        stdout = sys.stdout
        sys.stdout = file
        if name is not None:
            print(f'Cluster {name}, ', end='')
        print(f'{len(c):,} elements')
        mc = collections.Counter(classes[j] for j in c).most_common(nclasses)
        print(', '.join(f'{count:,} {avclass}' for avclass, count in mc),
                end='')
        nothers = len(c) - sum(count for avclass, count in mc)
        if nothers:
            print(f', {nothers:,} others')
        else:
            print()
        for name, (features, X, Xz) in numeric_features:
            avgs = np.nanmean(X[c], 0)
            stdevs = np.nanstd(X[c], 0)
            std_avgs = np.nanmean(Xz[c], 0)
            avgstr = ' '.join(f"\n{fn} {avg:,.2f}±{std:,.2f} [{std_avg:+,.2f}σ]"
                                 for fn, avg, std, std_avg
                                 in zip(features, avgs, stdevs, std_avgs))
            print(f"{name.capitalize()} centroid: {avgstr}")
        for name, processed in categorical_features:
            print(f"Top {name} features:")
            for fn, score, mean, std, homs in topfeatures(c, processed, nfeat):
                print(f' [{score:.2f}] {mean:.2f}±{std:.2f} {fn}')
        sys.stdout = stdout
    return describe

def vectorize_dicts(dicts, tfidf=True):
    v = DictVectorizer()
    X = v.fit_transform(dicts)
    nnz = X.getnnz(0)
    useful = np.where((nnz > 1) & (nnz < X.shape[0]))[0]
    X = X[:, useful]
    features = [v.feature_names_[f] for f in useful]
    # Choose between original categorical values vs normalized tf-idf values
    flen = len(features)
    XT = TfidfTransformer().fit_transform(X) if tfidf and flen > 0 else X
    return features, X, XT

def vectorize_iterables(iterables, tfidf=True):
    return vectorize_dicts([collections.Counter(iterable) \
            for iterable in iterables], tfidf)

def normalize_stdev(X):  # for dense matrices
    if len(X[0]) == 0 or not np.any(X):
        return X
    return (X - np.nanmean(X, axis=0)) / np.nanstd(X, axis=0)

def plotNumericField(f, norm=True, dist=True, hist=True):
    sns.set(color_codes=True)
    x = getField(all_data, f)
    x = np.array(x).astype(np.float)
    if norm:
        x = (x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)
    xmax = np.amax(x)

    if dist:
        fig = plt.figure(figsize=(10, 4))
        ax1 = fig.add_subplot(1, 2, 1)
        ax2 = fig.add_subplot(1, 2, 2)
        ax1.set_title('Distribution of ' + f)
        sns.distplot(x, ax=ax1)

        ax2.boxplot(x, vert=False, notch=True)
        ax2.set_title('Median of ' + f)
        plt.show()

    if hist:
        from scipy import stats
        fig = plt.figure(figsize=(10, 4))
        ax1 = fig.add_subplot(1, 2, 1)
        ax2 = fig.add_subplot(1, 2, 2)
        ax1.hist(x, bins='auto')
        ax1.set_title('Histogram of ' + f)

        res = stats.cumfreq(x, numbins=10)
        xy = res.lowerlimit + np.linspace(0,
                res.binsize*res.cumcount.size, res.cumcount.size)
        ax2.bar(xy, res.cumcount, width=res.binsize)
        ax2.set_title('Cumulative histogram of ' + f)
        ax2.set_xlim([xy.min(), xy.max()])
        plt.show()

    plt.close()

def getBasicNumStats(data, field_name, plot=True):
    data_list = getField(data, field_name)
    col_f = np.array([(float(x) if x != '-' else 0) for x in data_list])
    mean_val = np.mean(col_f)
    median_val = np.median(col_f)
    std_val = np.std(col_f)
    zeros = np.where(col_f == 0)
    nz = len(col_f[zeros]) * 100 / len(col_f)
    print(field_name + " (uniques: %d)\n\tzeros:\t%lf%%\n\tmedian:\t%lf\
            \n\tmean:\t%lf\n\tstd:\t%lf"
          % (len(collections.Counter(data_list)),\
                  nz, median_val, mean_val, std_val))
    if plot:
        plotNumericField(field_name)
    print("\n----------------------------------------------------------")


def getField(data, fieldname):
    field_list = []
    for d in data:
        field_list.append(d[fieldname])
    return field_list

def is_in_tranco(server_name=''):
    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
    return e2ld in tranco_list

def is_in_bgnoise(server_name=''):
    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
    return (e2ld in bg_e2ld_list) or (server_name in bg_fqdn_list)

def parse_features(features=''):
    num_feat = []
    cat_feat = []
    set_feat = []
    payload_feat = []
    client_n_feat = []
    client_c_feat = []
    server_n_feat = []
    server_c_feat = []
    cert_n_feat = []
    cert_c_feat = []
    with open(features) as f:
        for line in f:
            line_stripped = ','.join([e.strip() for e in line.split(',')])
            line_splitted = line_stripped.split(',')
            ftype = line_splitted[0]
            if ftype == "NUMERIC":
                num_feat.extend(line_splitted[1:])
            elif ftype == "CATEGORICAL":
                cat_feat.extend(line_splitted[1:])
            elif ftype == "SET":
                set_feat.extend(line_splitted[1:])
            elif ftype == "PAYLOAD":
                payload_feat.extend(line_splitted[1:])
            elif ftype == "CLIENT_N":
                client_n_feat.extend(line_splitted[1:])
            elif ftype == "CLIENT_C":
                client_c_feat.extend(line_splitted[1:])
            elif ftype == "SERVER_N":
                server_n_feat.extend(line_splitted[1:])
            elif ftype == "SERVER_C":
                server_c_feat.extend(line_splitted[1:])
            elif ftype == "CERT_N":
                cert_n_feat.extend(line_splitted[1:])
            elif ftype == "CERT_C":
                cert_c_feat.extend(line_splitted[1:])
    return (num_feat, cat_feat, set_feat,
            payload_feat, client_n_feat, client_c_feat,
            server_n_feat, server_c_feat, cert_n_feat, cert_c_feat)

def parse_dataset(options, args):
    data = []
    empty_filtered = set()
    benign_filtered = set()
    noise_filtered = set()
    counter = 0
    e_counter = 0
    b_counter = 0
    n_counter = 0
    with open(args[0]) as csv_file:
        csv_dict = csv.DictReader(csv_file, delimiter='\t')
        for line in csv_dict:
            # Parse only options.vectors connections
            if counter == options.vectors:
                break
            # Ignore not established/empty connections
            if not options.empty and \
                    (line['established'].strip() != 'True' \
                    or line['enc_data_size'].strip() == '0'):
                empty_filtered.add(line['server_name'])
                e_counter += 1
                continue
            # Filter benign connections according to a specific list
            if not options.benign=='' and \
                    is_in_tranco(line['server_name']):
                benign_filtered.add(line['server_name'])
                b_counter += 1
                continue
            # Filter background-noise connections
            if not options.noise and \
                    is_in_bgnoise(line['server_name']):
                noise_filtered.add(line['server_name'])
                n_counter += 1
                continue
            data.append(line)
            counter += 1
    if options.debug and not options.empty:
        print('Empty connections filtered: {}'.format(e_counter))
        print('Domain names: {}'.format(empty_filtered))
    if options.debug and not options.benign=='':
        print('Benign connections filtered: {}'.format(b_counter))
        print('Domain names: {}'.format(benign_filtered))
    if options.debug and not options.noise:
        print('Background-noise connections filtered: {}'.format(n_counter))
        print('Domain names: {}'.format(noise_filtered))
    return data

def main(options, args):
    global psl
    global tranco_list
    global bg_e2ld_list
    global bg_fqdn_list

    # Fetch PublicSuffix list and load it
    psl_file = pfetch()
    psl = PublicSuffixList(psl_file)

    # Load the tranco list
    if options.benign and len(tranco_list) == 0:
        with open(options.benign) as csv_file:
            for line in csv_file:
                # Add the domain name only
                domain_name = line.split(',')[1]
                tranco_list.add(domain_name.strip())

    # Load the background-noise lists
    if not options.noise and len(bg_e2ld_list) == 0 \
            and len(bg_fqdn_list) == 0:
        dlist = '{}/../data/background-noise/domainname-list'
        with open(dlist.format(path)) as listf:
            for line in listf:
                domain_name = line.strip()
                if domain_name.startswith('*'):
                    # Remove the *. at the beginning
                    bg_e2ld_list.add(domain_name[2:])
                else:
                    bg_fqdn_list.add(domain_name)

    # Get the data
    all_data = parse_dataset(options, args)
    # Get the feature sets
    NUMERIC_FEATURES, CAT_FEATURES, SET_FEATURES,\
            payload_features, client_n_features, client_c_features,\
            server_n_features, server_c_features, cert_n_features,\
            cert_c_features = parse_features(options.features)

    if options.debug:
        print('Processing {} vectors.'.format(len(all_data)))
        print('Numeric features: {}'.format(NUMERIC_FEATURES))
        print('Categorical features: {}'.format(CAT_FEATURES))
        print('Set features: {}'.format(SET_FEATURES))
        # TODO: What to do with plots?
        for n in NUMERIC_FEATURES:
            getBasicNumStats(all_data, n, False)

    # Remove from the features those fields with just one different value
    singulars = []
    for feature in NUMERIC_FEATURES+CAT_FEATURES:
        data_list = getField(all_data, feature)
        if len(collections.Counter(data_list)) == 1:
            singulars.append(feature)

    for feature in singulars:
        if options.debug:
            print("! Removing feature without information: {}".format(feature))
        if feature in NUMERIC_FEATURES:
            NUMERIC_FEATURES.remove(feature)
        if feature in CAT_FEATURES:
            CAT_FEATURES.remove(feature)
        if feature in payload_features:
            payload_features.remove(feature)
        if feature in client_n_features:
            client_n_features.remove(feature)
        if feature in client_c_features:
            client_c_features.remove(feature)
        if feature in server_n_features:
            server_n_features.remove(feature)
        if feature in server_c_features:
            server_c_features.remove(feature)
        if feature in cert_n_features:
            cert_n_features.remove(feature)
        if feature in cert_c_features:
            cert_c_features.remove(feature)

    if options.debug:
        print("PAYLOAD_N: {}".format(payload_features))
        print("CLIENT_N: {}".format(client_n_features))
        print("CLIENT_C: {}".format(client_c_features))
        print("SERVER_N: {}".format(server_n_features))
        print("SERVER_C: {}".format(server_c_features))
        print("CERT_N: {}".format(cert_n_features))
        print("CERT_C: {}".format(cert_c_features))

    num_features, cat_features, set_features, classes = [], [], [], []
    pay_feat, cli_n_feat, ser_n_feat, cer_n_feat = [], [], [], []
    cli_c_feat, ser_c_feat, cer_c_feat = [], [], []

    for d in all_data:
        # Numerical features
        # Fix for c_leaf_cert_validity = '-' (when no cert is found)
        num_features.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in NUMERIC_FEATURES])
        pay_feat.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in payload_features])
        cli_n_feat.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in client_n_features])
        ser_n_feat.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in server_n_features])
        cer_n_feat.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in cert_n_features])

        # Categorical features
        cat_tmp = []
        cli_tmp = []
        ser_tmp = []
        cer_tmp = []
        for v in SET_FEATURES:
            the_hash = str(d[v])
            cat_tmp.append(f'{v}_list:{the_hash}')
            for fn in d[v].split(','):
                cat_tmp.append(f'{v}:{fn}')

        for fn in CAT_FEATURES:
            cat_tmp.append(f'{fn}:{d[fn]}')
        for fn in client_c_features:
            cli_tmp.append(f'{fn}:{d[fn]}')
        for fn in server_c_features:
            ser_tmp.append(f'{fn}:{d[fn]}')
        for fn in cert_c_features:
            cer_tmp.append(f'{fn}:{d[fn]}')

        cat_features.append(cat_tmp)
        cli_c_feat.append(cli_tmp)
        ser_c_feat.append(ser_tmp)
        cer_c_feat.append(cer_tmp)

        classes.append(d['avclass_family'])

    X = np.array(num_features, dtype=float)
    #get_ipython().run_line_magic('time', 'X = np.array(num_features, dtype=float)')
    Xpay = np.array(pay_feat, dtype=float)
    Xcli = np.array(cli_n_feat, dtype=float)
    Xser = np.array(ser_n_feat, dtype=float)
    Xcer = np.array(cer_n_feat, dtype=float)

    # Normalization of numerical features
    Xz = normalize_stdev(X) if options.norm else X
    Xzpay = normalize_stdev(Xpay) if options.norm else Xpay
    Xzcli = normalize_stdev(Xcli) if options.norm else Xcli
    Xzser = normalize_stdev(Xser) if options.norm else Xser
    Xzcer = normalize_stdev(Xcer) if options.norm else Xcer

    # One-hot-encoding and TFIDF-transformation of categorical features
    cat_features, CX, CXT = vectorize_iterables(cat_features, options.tfidf)
    #get_ipython().run_line_magic('time', 'cat_features, CX, CXT = vectorize_iterables(cat_features)')
    cli_c_feat, CXcli, CXTcli = vectorize_iterables(cli_c_feat, options.tfidf)
    ser_c_feat, CXser, CXTser = vectorize_iterables(ser_c_feat, options.tfidf)
    cer_c_feat, CXcer, CXTcer = vectorize_iterables(cer_c_feat, options.tfidf)

    # we give to features a weight proportional to their number
    ALPHAN = len(NUMERIC_FEATURES) / \
        (len(NUMERIC_FEATURES) + len(CAT_FEATURES) + len(SET_FEATURES))
    ALPHAC = 1 - ALPHAN

    TOTFEAT = len(payload_features) + \
            len(client_n_features) + len(client_c_features) + \
            len(server_n_features) + len(server_c_features) + \
            len(cert_n_features) + len(cert_c_features)

    ALPHAP = len(payload_features) / TOTFEAT

    ALPHACLIN = len(client_n_features) / TOTFEAT
    ALPHACLIC = len(client_c_features) / TOTFEAT

    ALPHASERN = len(server_n_features) / TOTFEAT
    ALPHASERC = len(server_c_features) / TOTFEAT

    ALPHACERN = len(cert_n_features) / TOTFEAT
    ALPHACERC = len(cert_c_features) / TOTFEAT

    if options.debug:
        print("Numerical features: {}".format(len(NUMERIC_FEATURES)))
        print("Categorical features: {}".format(len(CAT_FEATURES)))
        print("Categorical features after vect: {}".format(len(cat_features)))
        print("Set features: {}".format(len(SET_FEATURES)))
        print("ALPHAN: {}\nALPHAC: {}".format(ALPHAN, ALPHAC))
        print("NUM TOTAL FEATURES: {}".format(TOTFEAT))
        print("ALPHAP: {}".format(ALPHAP))
        print("ALPHACLIN: {}\nALPHACLIC: {}".format(ALPHACLIN, ALPHACLIC))
        print("ALPHASERN: {}\nALPHASERC: {}".format(ALPHASERN, ALPHASERC))
        print("ALPHACERN: {}\nALPHACERC: {}".format(ALPHACERN, ALPHACERC))


    ##################### TODO: Check for consistency

    @numba.jit
    def distance(i, js):
        # Calculate categorical distances
        if options.distance == 'cosine' and options.ngroups:
            client_dc = cosine_distances(CXTcli[i], CXTcli[js])[0]\
                    if ALPHACLIC > 0 else 0
            server_dc = cosine_distances(CXTser[i], CXTser[js])[0]\
                    if ALPHASERC > 0 else 0
            cert_dc = cosine_distances(CXTcer[i], CXTcer[js])[0]\
                    if ALPHACERC > 0 else 0
        elif options.distance == 'cosine':
            cat_d = cosine_distances(CXT[i], CXT[js])[0]\
                    if ALPHAC > 0 else 0
        elif options.distance == 'euclidean' and options.ngroups:
            client_dc = euclidean_distances(CXTcli[i], CXTcli[js])[0]\
                    if ALPHACLIC > 0 else 0
            server_dc = euclidean_distances(CXTser[i], CXTser[js])[0]\
                    if ALPHASERC > 0 else 0
            cert_dc = euclidean_distances(CXTcer[i], CXTcer[js])[0]\
                    if ALPHACERC > 0 else 0
        elif options.distance == 'euclidean':
            cat_d = euclidean_distances(CXT[i], CXT[js])[0]\
                    if ALPHAC > 0 else 0

        # Calculate numeric distances
        if options.ngroups:
            payload_d = euclidean_distances(Xzpay[[i]], Xzpay[js])[0]\
                    if ALPHAP > 0 else 0
            client_dn = euclidean_distances(Xzcli[[i]], Xzcli[js])[0]\
                    if ALPHACLIN > 0 else 0
            server_dn = euclidean_distances(Xzser[[i]], Xzser[js])[0]\
                    if ALPHASERN > 0 else 0
            cert_dn = euclidean_distances(Xzcer[[i]], Xzcer[js])[0]\
                    if ALPHACERN > 0 else 0
        else:
            num_d = euclidean_distances(Xz[[i]], Xz[js])[0]\
                    if ALPHAN > 0 else 0

        if options.ngroups:
            if options.weight:
                payload_d = (ALPHAP*payload_d)
                client_d = (ALPHACLIC*client_dc) + (ALPHACLIN*client_dn)
                server_d = (ALPHASERC*server_dc) + (ALPHASERN*server_dn)
                cert_d = (ALPHACERC*cert_dc) + (ALPHACERN*cert_dn)
            else:
                client_d = client_dc + client_dn
                server_d = server_dc + server_dn
                cert_d = cert_dc + cert_dn
            # TODO: This works well for some thresholds, but it is tricky to
            # select it
            reduce_payload = isinstance(server_d, np.ndarray) \
                    and isinstance(cert_d, np.ndarray) \
                    and server_d.all() < options.threshold \
                    and cert_d.all() < options.threshold
            reduce_server = isinstance(cert_d, np.ndarray) \
                    and isinstance(payload_d, np.ndarray) \
                    and cert_d.all() < options.threshold \
                    and payload_d.all() < options.threshold
            reduce_cert = isinstance(server_d, np.ndarray) \
                    and isinstance(payload_d, np.ndarray) \
                    and server_d.all() < options.threshold \
                    and payload_d.all() < options.threshold
            payload_d = payload_d/2 if reduce_payload else payload_d
            server_d = server_d/2 if reduce_server else server_d
            cert_d = cert_d/2 if reduce_cert else cert_d
            # TODO: It is possible to calculate euclidean distance between payload,
            # client, server and cert resulting distances at each element?
            # return euclidean_distances()
#            print('i: {}; js: {}'.format(i, js))
#            print('Xz[[i]]: {}\nXz[js]: {}'.format(Xz[[i]], Xz[js]))
#            print('PAY_N: {}'.format(payload_d))
#            print('CLI_C: {}; CLI_N: {}'.format(client_dc, client_dn))
#            print('CER_C: {}; CER_N: {}'.format(cert_dc, cert_dn))
#            print('SER_C: {}; SER_N: {}'.format(server_dc, server_dn))
#            print('====================')
            return payload_d + client_d + server_d + cert_d
        else:
            if options.weight:
                num_d = (ALPHAN*num_d)
                cat_d = (ALPHAC*cat_d)
#            print('i: {}; js: {}'.format(i, js))
#            print('Xz[[i]]: {}\nXz[js]: {}'.format(Xz[[i]], Xz[js]))
#            print('CAT_D: {}; NUM_D: {}'.format(cat_d, num_d))
#            print('NUM_D + CAT_D: {}'.format(num_d + cat_d))
#            print('====================')
            return num_d + cat_d
            #return (ALPHAN*num_d) + (ALPHAC*cat_d)
            #return (ALPHA * euclidean_distances(Xz[[i]], Xz[js])[0]
            #    + (1 - ALPHA) * cosine_distances(CXT[i], CXT[js])[0])


    fishdbc = flexible_clustering.FISHDBC(distance,
            min_samples=options.mpts, m=options.m, ef=50, vectorized=True)

    r = len(all_data) if len(all_data) < options.vectors else options.vectors

    if options.debug:
        print('Clustering {} vectors'.format(r))

    r_min = 0
    r_max = r_step = 100000
    r_times = int(r/r_step)
    r_left = r % r_step

    for i in range(r_times):
        fishdbc.update(range(r_min, r_max))
        r_min = r_max
        r_max += r_step
    fishdbc.update(range(r_min, r_min+r_left))
    #%time fishdbc.update(range(100000, 500000))

    with open('tls_all_neighbor_heaps.txt', 'w') as nf:
        print(fishdbc._neighbor_heaps, file=nf)

    labels, probs, stabilities, condensed_tree, slt, mst = \
            results_full = fishdbc.cluster(min_cluster_size=options.mcs)
    #%time labels, probs, stabilities, condensed_tree, slt, mst = results_full = fishdbc.cluster(min_cluster_size=10)

    describe = describer(classes,
                             [("numeric", (NUMERIC_FEATURES, X, Xz))],
                             [("categorical", (cat_features, CX, CXT))])

    clusters = get_clusters(labels)


    if options.debug:
        print('Clusters in the most common(20) list:')
        for label, total in collections.Counter(labels).most_common(20):
        #    describe(clusters[label], label)
            print('\t {}:\t{}'.format(label, total))

    with open('tls_all_flat_clustering_fishdbc.txt', 'w') as f:
        for label, c in sorted(clusters.items()):
            describe(c, label, file=f)
            print(file=f)

    # TODO change format to better see the condensed tree
    with open('tls_all_condensed_tree.txt', 'w') as ct:
        h = get_hierarchy(condensed_tree, skipping_ratio=0.9, file=ct)
    #get_ipython().run_line_magic('time', 'h = get_hierarchy(condensed_tree, skipping_ratio=0.9)')
    hierarchy = h['hierarchy']
    hclusters = h['clusters']
    hlambdas = h['lambdas']

    lineages = sorted(get_lineage(c, hierarchy) for c in hclusters)
    #get_ipython().run_line_magic('time', 'lineages = sorted(lineage(c, hierarchy) for c in hclusters)')

    with open('tls_all_hier_clustering_fishdbc.txt', 'w') as f:
        for lineage in lineages:
            describe(hclusters[lineage[-1]], '.'.join(map(str, lineage)),
                     file=f)
            print(file=f)

    ids = []
    for d in all_data:
        ids.append(d['sha2']+":"+d['conn_uid'])

    jsonflat = asjson(clusters, ids, classes,
                          [("numeric", (NUMERIC_FEATURES, X, Xz))],
                          [("categorical", (cat_features, CX, CXT))])

    with open('tls_all_flat_clustering_fishdbc.json', 'w') as f:
        json.dump(jsonflat, f)

    # Write the flat results for the scoring
    singletons = {}
    last_cluster_label = len(jsonflat) - 1
    with open('tls_all_flat_results.csv', 'w') as f:
        for c in jsonflat:
            for idc in c['ids']:
                if options.singletons and c['label'] == -1:
                    singletons.update({idc: last_cluster_label})
                    last_cluster_label += 1
                else:
                    print("{},{}".format(idc, c['label']), file=f)
        if options.singletons:
            for idc, clabel in singletons.items():
                print("{},{}".format(idc, clabel), file=f)

    jsonhier = asjson(hclusters, ids, classes,
                          [("numeric", (NUMERIC_FEATURES, X, Xz))],
                          [("categorical", (cat_features, CX, CXT))],
                          lineages=lineages, lambdas=hlambdas)

    with open('tls_all_hier_clustering_fishdbc.json', 'w') as f:
        json.dump(jsonhier, f)

    if options.debug:
        print('Clustering done.')


if __name__ == '__main__':
    parser = optparse.OptionParser(
            usage="Usage: %prog [options] dataset_filename",
            version="%prog 1.0")
    parser.add_option(
            '-f', '--features', action='store', dest='features',
            help='Path to the file with the features set in csv format',
            type='str')
    parser.add_option(
            '-v', '--vectors', action='store', dest='vectors',
            help='Maximum number of vectors to process',
            type='int', default=10000)
    parser.add_option(
            '-b', '--benign', action='store', dest='benign',
            help='Filter benign connections using a specific CSV list',
            type=str, default='')
    parser.add_option(
            '-B', '--background-noise', action='store_true', dest='noise',
            help='Cluster background-noise connections (do not filter them)',
            default=False)
    parser.add_option(
            '-e', '--empty', action='store_true', dest='empty',
            help='Cluster vectors without payload (ApplicationData) packages',
            default=False)
    parser.add_option(
            '-s', '--singletons', action='store_true', dest='singletons',
            help='Convert outliers into singletons',
            default=False)
    parser.add_option(
            '-c', '--mcs', action='store', dest='mcs',
            help='Minimum cluster size',
            type='int', default=10)
    parser.add_option(
            '-p', '--mpts', action='store', dest='mpts',
            help='Number of nodes that define a dense zone',
            type='int', default=10)
    parser.add_option(
            '-m', '--m', action='store', dest='m',
            help='Number of neighbors per layer in HNSW',
            type='int', default=10)
    parser.add_option(
            '-n', '--norm', action='store_true', dest='norm',
            help='Normalize numeric values using z-score',
            default=False)
    parser.add_option(
            '-t', '--tfidf', action='store_true', dest='tfidf',
            help='Transform categorical values using TFIDF',
            default=False)
    parser.add_option(
            '-d', '--distance', action='store', dest='distance',
            type='choice', choices=['euclidean', 'cosine'],
            help='Distance function for categorical values',
            default='cosine')
    parser.add_option(
            '-D', '--debug', action='store_true', dest='debug',
            help='Print debug messages',
            default=False)
    parser.add_option(
            '-w', '--weight', action='store_true', dest='weight',
            help='Give weight to groups of features according to their number',
            default=False)
    parser.add_option(
            '-g', '--new-groups', action='store_true', dest='ngroups',
            help='Use the new division of feature groups',
            default=False)
    parser.add_option(
            '-T', '--threshold', action='store', dest='threshold',
            help='Threshold to reduce the distance between server, cert' +
            ' and payload groups of features (only works when using -g)',
            type='float', default=0.0)

    options, args = parser.parse_args()

    if len(args) != 1 or not os.path.isfile(args[0]):
        parser.error("Dataset not found. Exiting.")
    if not os.path.isfile(options.features):
        parser.error("File of features not found. Exiting.")
    if options.benign and not os.path.isfile(options.benign):
        parser.error("File for filter benign connections not found. Exiting.")

    main(options, args)
