#!/usr/bin/env python
# coding: utf-8

import os
import sys
import csv
import json
import hashlib
import optparse
from collections import Counter, defaultdict
path = os.path.dirname(os.path.abspath(__file__))
libpath = os.path.join(path, '../lib/')
sys.path.insert(0, libpath)
import grouping

def parse_features(features=''):
    payload_feat = []
    with open(features) as f:
        for line in f:
            line_stripped = ','.join([e.strip() for e in line.split(',')])
            line_splitted = line_stripped.split(',')
            ftype = line_splitted[0]
            if ftype == "PAYLOAD":
                payload_feat.extend(line_splitted[1:])
    return payload_feat

def parse_groups_dataset(cfp_path, dataset):
    groups = defaultdict(list)
    data = {}
    cids = set()
    # Load groups by client fingerprint
    with open(cfp_path) as pf:
        for line in pf:
            pline = line.strip().split(',')
            cid = pline[0]
            label = int(pline[1])
            cids.add(cid)
            groups[label].append(cid)
    # Load the dataset
    with open(dataset) as csv_file:
        csv_dict = csv.DictReader(csv_file, delimiter='\t')
        for elem in csv_dict:
            # Take groups-clustered connections only
            eid = '{}:{}'.format(elem['sha2'], elem['conn_uid'])
            if eid in cids:
                data.update({eid: elem})
    return (groups, data)

def yield_content_hashes(groups, data, simf):
    hashes = {}
    empty = set()
    for c, ids in groups.items():
        c_items = {}
        empty_group = True
        for id in ids:
            empty_id = data[id]['enc_data_size'] == '0'
            empty_group &= empty_id
            if empty_id:
                c_items[id] = '0'
                continue
            # Generating hash from content features
            content = ';'.join(data[id][f] for f in simf)
            m = hashlib.md5()
            m.update(content.encode('utf-8'))
            c_items[id] = m.hexdigest()
            if options.debug:
                print('ID {}\nMD5: {}'.format(id, c_items[id]))
        hashes[c] = c_items
        if empty_group:
            empty.add(c)
    return (hashes, empty)

def compare_hashes(d1, d2):
    hashes_g1 = set(d1.values())
    hashes_g2 = set(d2.values())
    if options.single and (len(hashes_g1) > 1 or len(hashes_g2) > 1):
        return False
    if options.aggressive:
        return hashes_g1.intersection(hashes_g2)
    else:
        return hashes_g1 == hashes_g2

def search_group(c, e, groups):
    for x, xset in groups.items():
        if e in xset:
            return x
    return c

def merge_clusters(merge_these, hashes):
    for c, cset in merge_these.items():
        items = hashes.pop(c)
        for e in cset:
            items.update(hashes.pop(e))
        hashes[c] = items
    return hashes

#def output_as_json(clusters, classes):
#    res = []
#    for label, cids in sorted(clusters.items()):
#        idclasses = Counter(classes[i] for i in cids).most_common()
#        current = {
#            'label': int(label),
#            'ids': cids,
#            'avclass': idclasses,
#            'size': len(cids),
#        }
#        res.append(current)
#    return res

def main(options, args):
    # Parse groups by FINGERPRINT and original dataset
    groups, data = parse_groups_dataset(options.groups, args[0])

    if options.debug:
        print('Pre-processing {} vectors.'.format(len(data)))

    # Defining similarity fields
    simf = parse_features(options.features)
    if options.cipher:
        simf.append('s_cipher')

    groups_cnt_hashes, empty = yield_content_hashes(groups, data, simf)

    omit = set([e for e in groups_cnt_hashes if e in empty])
    print('OMIT THESE: {}'.format(omit))
    merge_these = defaultdict(set)
    for c, ids in sorted(groups_cnt_hashes.items()):
        if c in omit:
            continue
        omit.add(c)
        if options.aggressive:
            g = search_group(c, c, merge_these)
        tocheck = sorted([x for x in groups_cnt_hashes if x not in omit])
        for e in tocheck:
            if compare_hashes(ids, groups_cnt_hashes[e]):
                if options.aggressive:
                    gg = search_group(g, e, merge_these)
                    if gg != g:
                        merge_these[g].add(g)
                        g_list = merge_these.pop(g)
                        merge_these[gg].update(g_list)
                        g = gg
                    else:
                        merge_these[g].add(e)
                else:
                    merge_these[c].add(e)
                    omit.add(e)
                if options.debug:
                    print('Found same group of content hashes:')
                    g1 = sorted(set(ids.values()))
                    g2 = sorted(set(groups_cnt_hashes[e].values()))
                    print('{} <-> {}'.format(g1, g2))
    print('MERGE_THESE: {}'.format(merge_these))

    groups_merged = merge_clusters(merge_these, groups_cnt_hashes)

    output_file = 'tls_clustering_grouping_cnt.tsv'
    output_json = 'tls_clustering_grouping_cnt.json'

    labels = []
    with open(output_file, 'w') as of:
        classes = {}
        clusters = defaultdict(list)
        for new_label, (label, ids) in enumerate(groups_merged.items()):
            clusters[new_label] = [id for id in ids]
            for id, chash in ids.items():
                print('{},{},{}'.format(id, new_label, chash), file=of)
                labels.append(new_label)
                classes[id] = data[id]['avclass_family']

    with open(output_json, 'w') as oj:
        json_out = grouping.output_as_json(clusters, classes)
        json.dump(json_out, oj)

    if options.debug:
        print('Groups in the most common(20) list:')
        for label, total in Counter(labels).most_common(20):
            print('\t{}:\t{}'.format(label, total))

    if options.debug:
        print('Grouping by content, done.')


if __name__ == '__main__':
    parser = optparse.OptionParser(
            usage="Usage: %prog [options] dataset_filename",
            version="%prog 1.0")
    parser.add_option(
            '-g', '--groups', action='store', dest='groups',
            help='Path to the file containing the groups by Client Fingerprint')
    parser.add_option(
            '-f', '--features', action='store', dest='features',
            help='Path to the file containing the features set')
    parser.add_option(
            '-s', '--single', action='store_true', dest='single',
            help='Merge groups only when there is a single hash in each',
            default=False)
    parser.add_option(
            '-a', '--aggressive', action='store_true', dest='aggressive',
            help='Merge two groups if at least one hash is present in both',
            default=False)
    parser.add_option(
            '-c', '--include-cipher', action='store_true', dest='cipher',
            help='Include the cipher suite in the hash generation',
            default=False)
    parser.add_option(
            '-D', '--debug', action='store_true', dest='debug',
            help='Print debug messages',
            default=False)

    options, args = parser.parse_args()

    if len(args) != 1 or not os.path.isfile(args[0]):
        parser.error("Dataset not found. Aborting...")
    if not os.path.isfile(options.groups):
        parser.error("File with groups by SNI not found. Aborting...")

    main(options, args)
