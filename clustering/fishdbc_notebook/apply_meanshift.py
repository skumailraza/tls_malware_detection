#!/usr/bin/env python
# coding: utf-8

import os
import sys
import csv
import json
import numpy as np
import optparse
from collections import defaultdict, Counter
from publicsuffixlist import PublicSuffixList
from publicsuffix import fetch as pfetch
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.cluster import MeanShift, estimate_bandwidth
path = os.path.dirname(os.path.abspath(__file__))
libpath = os.path.join(path, '../lib/')
sys.path.insert(0, libpath)
import extract_tld_esld

# Global variables
psl = object()
tranco_list = set()
bg_e2ld_list = set()
bg_fqdn_list = set()

def asjson(clusters, ids, classes, numeric, categorical, top_categories=20):
    res = []
    for label, c in sorted(clusters.items()):
        current = {
            'label': int(label),
            'ids': [ids[i] for i in c],
            'avclass': Counter(classes[i] for i in c).most_common(),
            'size': len(c),
        }
        for name, (features, X, Xz) in numeric:
            avgs = np.nanmean(X[c], 0)
            stdevs = np.nanstd(X[c], 0)
            homs = [stdevs[i]/avgs[i] if avgs[i] > 0 else 0 \
                    for i in range(len(features))]
            std_avgs = np.nanmean(Xz[c], 0)

            current[f'{name}_hasvalue'] = len(c)
            current[f'{name}_centroid'] = \
                    [{'name': fn, 'avg': avg, 'stdev': std,
                        'std_avg': std_avg, 'homs': homs}
                        for fn, avg, std, std_avg, homs
                        in zip(features, avgs, stdevs, std_avgs, homs)]
            current[f'{name}_inhomogeneity'] = np.sum(homs) / len(features)\
                    if len(features) > 0 else -1
        for name, processed in categorical:
            current[f'{name}_hasvalue'] = len(c)
            current[f'top_{name}'] = \
                    [{'name': fn, 'score': score, 'mean': mean,
                        'stdev': std, 'homs': homs}
                        for fn, score, mean, std, homs
                        in topfeatures(c, processed, top_categories)]
            homs = [current[f'top_{name}'][i]['homs']
                    for i in range(len(current[f'top_{name}']))]
            current[f'{name}_inhomogeneity'] = np.sum(homs) / len(homs)\
                    if len(homs) > 0 else -1
        res.append(current)
    return res

def get_clusters(labels):
    clusters = defaultdict(list)
    for i, l in enumerate(labels):
        clusters[l].append(i)
    return clusters

def topfeatures(c, processed, nfeat=5):
    featurenames, X, XT = processed
    avgT = XT[c].mean(0).A1
    top = avgT.argsort()[:-nfeat-1:-1]
    Xc = X[c]
    for f in top:
        score = avgT[f]
        if score <= 0:
            break
        Xcf = Xc[:, f].todense()
        xmean = Xcf.mean()
        xstdv = Xcf.std()
        xhoms = xstdv/xmean if xmean > 0 else 0
        yield featurenames[f], score, xmean, xstdv, xhoms

def describer(classes, numeric_features, categorical_features):
    def describe(c, name=None, nclasses=5, nfeat=5, file=sys.stdout):
        stdout = sys.stdout
        sys.stdout = file
        if name is not None:
            print(f'Cluster {name}, ', end='')
        print(f'{len(c):,} elements')
        mc = Counter(classes[j] for j in c).most_common(nclasses)
        print(', '.join(f'{count:,} {avclass}' for avclass, count in mc),
                end='')
        nothers = len(c) - sum(count for avclass, count in mc)
        if nothers:
            print(f', {nothers:,} others')
        else:
            print()
        for name, (features, X, Xz) in numeric_features:
            avgs = np.nanmean(X[c], 0)
            stdevs = np.nanstd(X[c], 0)
            std_avgs = np.nanmean(Xz[c], 0)
            avgstr = ' '.join(f"\n{fn} {avg:,.2f}±{std:,.2f} [{std_avg:+,.2f}σ]"
                                 for fn, avg, std, std_avg
                                 in zip(features, avgs, stdevs, std_avgs))
            print(f"{name.capitalize()} centroid: {avgstr}")
        for name, processed in categorical_features:
            print(f"Top {name} features:")
            for fn, score, mean, std, homs in topfeatures(c, processed, nfeat):
                print(f' [{score:.2f}] {mean:.2f}±{std:.2f} {fn}')
        sys.stdout = stdout
    return describe

def vectorize_dicts(dicts, tfidf=True):
    v = DictVectorizer()
    X = v.fit_transform(dicts)
    nnz = X.getnnz(0)
    useful = np.where((nnz > 1) & (nnz < X.shape[0]))[0]
    X = X[:, useful]
    features = [v.feature_names_[f] for f in useful]
    # Choose between original categorical values vs normalized tf-idf values
    flen = len(features)
    XT = TfidfTransformer().fit_transform(X) if tfidf and flen > 0 else X
    return features, X, XT

def vectorize_iterables(iterables, tfidf=True):
    return vectorize_dicts([Counter(iterable) \
            for iterable in iterables], tfidf)

def normalize_stdev(X):  # for dense matrices
    if len(X[0]) == 0 or not np.any(X):
        return X
    return (X - np.nanmean(X, axis=0)) / np.nanstd(X, axis=0)

def getField(data, fieldname):
    field_list = []
    for d in data:
        field_list.append(d[fieldname])
    return field_list

def is_in_tranco(server_name=''):
    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
    return e2ld in tranco_list

def is_in_bgnoise(server_name=''):
    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
    return (e2ld in bg_e2ld_list) or (server_name in bg_fqdn_list)

def parse_features(features=''):
    num_feat = []
    cat_feat = []
    with open(features) as f:
        for line in f:
            line_stripped = ','.join([e.strip() for e in line.split(',')])
            line_splitted = line_stripped.split(',')
            ftype = line_splitted[0]
            if ftype == "NUMERIC":
                num_feat.extend(line_splitted[1:])
            elif ftype == "CATEGORICAL":
                cat_feat.extend(line_splitted[1:])
    return (num_feat, cat_feat)

def parse_dataset(options, args):
    data = []
    empty_filtered = set()
    benign_filtered = set()
    noise_filtered = set()
    counter = 0
    e_counter = 0
    b_counter = 0
    n_counter = 0
    with open(args[0]) as csv_file:
        csv_dict = csv.DictReader(csv_file, delimiter='\t')
        for line in csv_dict:
            # Parse only options.vectors connections
            if counter == options.vectors:
                break
            # Ignore not established/empty connections
            if not options.empty and \
                    (line['established'].strip() != 'True' \
                    or line['enc_data_size'].strip() == '0'):
                empty_filtered.add(line['server_name'])
                e_counter += 1
                continue
            # Filter benign connections according to a specific list
            if not options.benign=='' and \
                    is_in_tranco(line['server_name']):
                benign_filtered.add(line['server_name'])
                b_counter += 1
                continue
            # Filter background-noise connections
            if not options.noise and \
                    is_in_bgnoise(line['server_name']):
                noise_filtered.add(line['server_name'])
                n_counter += 1
                continue
            data.append(line)
            counter += 1
    if options.debug and not options.empty:
        print('Empty connections filtered: {}'.format(e_counter))
        print('Domain names: {}'.format(empty_filtered))
    if options.debug and not options.benign=='':
        print('Benign connections filtered: {}'.format(b_counter))
        print('Domain names: {}'.format(benign_filtered))
    if options.debug and not options.noise:
        print('Background-noise connections filtered: {}'.format(n_counter))
        print('Domain names: {}'.format(noise_filtered))
    return data

def main(options, args):
    global psl
    global tranco_list
    global bg_e2ld_list
    global bg_fqdn_list

    # Fetch PublicSuffix list and load it
    psl_file = pfetch()
    psl = PublicSuffixList(psl_file)

    # Load the tranco list
    if options.benign and len(tranco_list) == 0:
        with open(options.benign) as csv_file:
            for line in csv_file:
                # Add the domain name only
                domain_name = line.split(',')[1]
                tranco_list.add(domain_name.strip())

    # Load the background-noise lists
    if not options.noise and len(bg_e2ld_list) == 0 \
            and len(bg_fqdn_list) == 0:
        dlist = '{}/../data/background-noise/domainname-list'
        with open(dlist.format(path)) as listf:
            for line in listf:
                domain_name = line.strip()
                if domain_name.startswith('*'):
                    # Remove the *. at the beginning
                    bg_e2ld_list.add(domain_name[2:])
                else:
                    bg_fqdn_list.add(domain_name)

    # Get the data
    all_data = parse_dataset(options, args)
    # Get the numeric features
    NUMERIC_FEATURES, CAT_FEATURES = parse_features(options.features)

    if options.debug:
        print('Processing {} vectors.'.format(len(all_data)))
        print('Numeric features: {}'.format(NUMERIC_FEATURES))
        print('Categorical features: {}'.format(CAT_FEATURES))

    # Remove from the features those fields with just one different value
    singulars = []
    for feature in NUMERIC_FEATURES+CAT_FEATURES:
        data_list = getField(all_data, feature)
        if len(Counter(data_list)) == 1:
            singulars.append(feature)

    for feature in singulars:
        if options.debug:
            print("! Removing feature without information: {}".format(feature))
        if feature in NUMERIC_FEATURES:
            NUMERIC_FEATURES.remove(feature)
        if feature in CAT_FEATURES:
            CAT_FEATURES.remove(feature)

    if options.debug:
        print("NUMERIC_FEATURES: {}".format(NUMERIC_FEATURES))

    num_features, cat_features, classes = [], [], []

    for d in all_data:
        # Numerical features
        # Fix for c_leaf_cert_validity = '-' (when no cert is found)
        num_features.append([(float(d[f]) if d[f] != '-' else 0) \
                for f in NUMERIC_FEATURES])

        cat_tmp = []
        for fn in CAT_FEATURES:
            cat_tmp.append(f'{fn}:{d[fn]}')

        cat_features.append(cat_tmp)
        classes.append(d['avclass_family'])

    X = np.array(num_features, dtype=float)

    # Normalization of numerical features
    Xz = normalize_stdev(X) if options.norm else X

    # Normalization of categorical features
    cat_features, CX, CXT = vectorize_iterables(cat_features, True)

    # Join matrices
    if cat_features and num_features:
        cxt_dense = np.array(CXT.todense())
        vectors = np.concatenate((Xz, cxt_dense), axis=1)
    elif cat_features:
        vectors = np.array(CXT.todense())
    else:
        vectors = Xz

    r = len(all_data) if len(all_data) < options.vectors else options.vectors

    if options.debug:
        print('Clustering {} vectors'.format(r))

    ##########################################################################
    # MeanShift clustering
    if not options.bandwidth:
        print('Calculating bandwidth.')
        bandwidth = estimate_bandwidth(vectors, quantile=0.3)
        print('Sugested value: {}'.format(bandwidth))
    else:
        bandwidth = options.bandwidth
#    ms = MeanShift(bandwidth=bandwidth, cluster_all=False)
##   bw=0.8 # Good!, though gathers up00/bu01/ra00 and bu00/bu02
##   bw=0.7 # Better, gathers up00/bu01 and bu00/bu02 but not ra00
##   bw=0.6 # Still gathers up00/bu01 and bu00/bu02
##   bw=0.5 # Still gathers up00/bu01 and some bu00 with bu02, too specific
##   bw=0.1 # Still gathers same conns, with too specific singletons
    ms = MeanShift(bandwidth=bandwidth, cluster_all=False)
#    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
    ms.fit(vectors)
    labels = ms.labels_
    ##########################################################################

    describe = describer(classes, [("numeric", (NUMERIC_FEATURES, X, Xz))],
            [("categorical", (cat_features, CX, CXT))])

    clusters = get_clusters(labels)

    if options.debug:
        print('Clusters in the most common(20) list:')
        for label, total in Counter(labels).most_common(20):
            print('\t {}:\t{}'.format(label, total))

    with open('tls_clustering_meanshift.txt', 'w') as f:
        for label, c in sorted(clusters.items()):
            describe(c, label, file=f)
            print(file=f)

    ids = [d['sha2']+":"+d['conn_uid'] for d in all_data]

    jsonflat = asjson(clusters, ids, classes,
                [("numeric", (NUMERIC_FEATURES, X, Xz))],
                [("categorical", (cat_features, CX, CXT))])

    with open('tls_clustering_meanshift.json', 'w') as f:
        json.dump(jsonflat, f)

    # Write the flat results for the scoring
    singletons = {}
    last_cluster_label = len(jsonflat) - 1
    with open('tls_clustering_meanshift_results.csv', 'w') as f:
        for c in jsonflat:
            for idc in c['ids']:
                if options.singletons and c['label'] == -1:
                    singletons.update({idc: last_cluster_label})
                    last_cluster_label += 1
                else:
                    print("{},{}".format(idc, c['label']), file=f)
        if options.singletons:
            for idc, clabel in singletons.items():
                print("{},{}".format(idc, clabel), file=f)

    if options.debug:
        print('Meanshift clustering done.')


if __name__ == '__main__':
    parser = optparse.OptionParser(
            usage="Usage: %prog [options] dataset_filename",
            version="%prog 1.0")
    parser.add_option(
            '-f', '--features', action='store', dest='features',
            help='Path to the file with the features set in csv format',
            type='str')
    parser.add_option(
            '-v', '--vectors', action='store', dest='vectors',
            help='Maximum number of vectors to process',
            type='int', default=10000)
    parser.add_option(
            '-b', '--benign', action='store', dest='benign',
            help='Filter benign connections using a specific CSV list',
            type=str, default='')
    parser.add_option(
            '-B', '--background-noise', action='store_true', dest='noise',
            help='Cluster background-noise connections (do not filter them)',
            default=False)
    parser.add_option(
            '-e', '--empty', action='store_true', dest='empty',
            help='Cluster vectors without payload (ApplicationData) packages',
            default=False)
    parser.add_option(
            '-s', '--singletons', action='store_true', dest='singletons',
            help='Convert outliers into singletons',
            default=False)
    parser.add_option(
            '-w', '--bandwidth', action='store', dest='bandwidth',
            help='Specify a bandwidth for MeanShift instead of calculating it',
            type='float', default=0.0)
    parser.add_option(
            '-n', '--norm', action='store_true', dest='norm',
            help='Normalize numeric values using z-score',
            default=False)
    parser.add_option(
            '-D', '--debug', action='store_true', dest='debug',
            help='Print debug messages',
            default=False)

    options, args = parser.parse_args()

    if len(args) != 1 or not os.path.isfile(args[0]):
        parser.error("Dataset not found. Exiting.")
    if not os.path.isfile(options.features):
        parser.error("File of features not found. Exiting.")
    if options.benign and not os.path.isfile(options.benign):
        parser.error("File for filter benign connections not found. Exiting.")

    main(options, args)
