#!/usr/bin/env python
# coding: utf-8

import re
import os
import sys
import csv
import json
import optparse
from collections import Counter, defaultdict
from publicsuffixlist import PublicSuffixList
from publicsuffix import fetch as pfetch
path = os.path.dirname(os.path.abspath(__file__))
libpath = os.path.join(path, '../lib/')
sys.path.insert(0, libpath)
#import extract_tld_esld
import grouping

# Global variables
#psl = object()
#tranco_list = set()
#bg_e2ld_list = set()
#bg_fqdn_list = set()
# Tor-like subject and issuer
torsubcn = "^www\.[a-z2-9]{8,20}\.net$"
torisscn = "^www\.[a-z2-9]{8,20}\.com$"

#def is_in_tranco(server_name=''):
#    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
#    return e2ld in tranco_list
#
#def is_in_bgnoise(server_name=''):
#    tld, e2ld = extract_tld_esld.get_tld_esld(psl, server_name)
#    return (e2ld in bg_e2ld_list) or (server_name in bg_fqdn_list)
#
#def parse_dataset(options, args):
#    data = []
#    empty_filtered = set()
#    benign_filtered = set()
#    noise_filtered = set()
#    counter = 0
#    e_counter = 0
#    b_counter = 0
#    n_counter = 0
#    with open(args[0]) as csv_file:
#        csv_dict = csv.DictReader(csv_file, delimiter='\t')
#        for line in csv_dict:
#            # Parse only options.vectors connections
#            if counter == options.vectors:
#                break
#            # Ignore not established/empty connections
#            if not options.empty and \
#                    (line['established'].strip() != 'True' \
#                    or line['enc_data_size'].strip() == '0'):
#                empty_filtered.add(line['server_name'])
#                e_counter += 1
#                continue
#            # Filter benign connections according to a specific list
#            if not options.benign=='' and \
#                    is_in_tranco(line['server_name']):
#                benign_filtered.add(line['server_name'])
#                b_counter += 1
#                continue
#            # Filter background-noise connections
#            if not options.noise and \
#                    is_in_bgnoise(line['server_name']):
#                noise_filtered.add(line['server_name'])
#                n_counter += 1
#                continue
#            data.append(line)
#            counter += 1
#    if options.debug and not options.empty:
#        print('Empty connections filtered: {}'.format(e_counter))
#        print('Domain names: {}'.format(empty_filtered))
#    if options.debug and not options.benign=='':
#        print('Benign connections filtered: {}'.format(b_counter))
#        print('Domain names: {}'.format(benign_filtered))
#    if options.debug and not options.noise:
#        print('Background-noise connections filtered: {}'.format(n_counter))
#        print('Domain names: {}'.format(noise_filtered))
#    return data
#
#def output_as_json(clusters, classes):
#    res = []
#    for label, cids in sorted(clusters.items()):
#        idclasses = Counter(classes[i] for i in cids).most_common()
#        current = {
#            'label': int(label),
#            'ids': cids,
#            'avclass': idclasses,
#            'size': len(cids),
#        }
#        res.append(current)
#    return res

def is_tor(subjcn, isscn):
    # Looking for Tor-like connections
    is_tor_subcn = re.search(torsubcn, subjcn)
    is_tor_isscn = re.search(torisscn, isscn)
    return is_tor_subcn and is_tor_isscn

def main(options, args):
#    global psl
#    global tranco_list
#    global bg_e2ld_list
#    global bg_fqdn_list

    # Fetch PublicSuffix list and load it
    psl_file = pfetch()
    psl = PublicSuffixList(psl_file)

    # Load the tranco list
#    if not options.benign=='' and len(tranco_list) == 0:
#        with open(options.benign) as csv_file:
#            for line in csv_file:
#                # Add the domain name only
#                domain_name = line.split(',')[1]
#                tranco_list.add(domain_name.strip())

    # Load the background-noise lists
#    if not options.noise and len(bg_e2ld_list) == 0 \
#            and len(bg_fqdn_list) == 0:
#        dlist = '{}/../data/background-noise/domainname-list'
#        with open(dlist.format(path)) as listf:
#            for line in listf:
#                domain_name = line.strip()
#                if domain_name.startswith('*'):
#                    # Remove the *. at the beginning
#                    bg_e2ld_list.add(domain_name[2:])
#                else:
#                    bg_fqdn_list.add(domain_name)

    # Get the data
#    all_data = parse_dataset(options, args)
    # TODO: benign_seq contains all sequences that must be reported to the
    # content grouping, in order to filter them there to keep only those of
    # malware (and to reduce FPR)
    all_data, benign_seq = grouping.parse_dataset(options.benign, options.noise,
            options.empty, options.vectors, options.debug, args[0])

    if options.debug:
        print('Processing {} vectors.'.format(len(all_data)))

    tor = {}
    tor_samples = set()
    classes = {}
    server_names = defaultdict(list)
    for d in all_data:
        id = '{}:{}'.format(d['sha2'], d['conn_uid'])
        classes[id] = d['avclass_family']
        # Looking for Tor-like connections
        if is_tor(d['s_leaf_cert_subj_cn'], d['s_leaf_cert_iss_cn']):
            tor[id] = {'ip': d['s_dst_ip'], 'fam': d['avclass_family']}
            tor_samples.add(d['sha2'])
            if options.tor:
                server_names['tor'].append(id)
                continue
        # Do not group by server_name when is NULL, but by dst_ip
        # TODO look for DNS records
        if d['server_name'] == 'NULL':
            server_names[d['s_dst_ip']].append(id)
        else:
#            tld, esld = extract_tld_esld.get_tld_esld(psl, d['server_name'])
            tld, esld = grouping.tld_esld(d['server_name'])
            server_names[esld].append(id)

    output_file = 'tls_clustering_grouping_sni.tsv'
    output_json = 'tls_clustering_grouping_sni.json'
    output_tor_file = 'tls_clustering_grouping_tor.tsv'
    output_benign_seq = 'tls_clustering_grouping_benign_seq.json'

    labels = []
    with open(output_file, 'w') as of:
        clusters = defaultdict(list)
        server_names_ord = sorted(server_names.items())
        for label, (sni, ids) in enumerate(server_names_ord):
            clusters[label].extend(ids)
            for id in ids:
                print('{},{},{}'.format(id, label, sni), file=of)
                labels.append(sni)

    with open(output_json, 'w') as oj:
        json_out = grouping.output_as_json(clusters, classes)
        json.dump(json_out, oj)

    with open(output_tor_file, 'w') as ot:
        output = '\n'
        ips = defaultdict(set)
        fams = defaultdict(list)
        for id, d in sorted(tor.items(),
                key=lambda x: (x[1]['fam'], x[1]['ip'])):
            ips[d['ip']].add(d['fam'])
            fams[d['fam']].append(d['ip'])
            output += '{},{},{}\n'.format(id, d['ip'], d['fam'])
        p = len(tor) * 100 / len(all_data) if len(all_data) else 0
        tconns = '# Found {} Tor-like connections ({:.4f}%) of the total'
        tconns = tconns.format(len(tor), p)
        print(tconns, file=ot)
        print('# Number of samples: {}'.format(len(tor_samples)), file=ot)
        print('# Different IPs: {}'.format(len(ips)), file=ot)
        print('# Different families found: {}'.format(len(fams)), file=ot)
        for f, fip in sorted(fams.items(), key=lambda x: -len(x[1])):
            p = len(fip) * 100 / len(tor) if len(tor) else 0
            s = set(fip)
            print('#\t {}: {} conns ({:.4f}%)'.format(f, len(fip), p), file=ot)
            print('#\t\t IPs: {}'.format(len(s)), file=ot)
            print('#\t\t\t ' + ', '.join(s), file=ot)
        ipsvar = {x: ips[x] for x in ips if len(ips[x]) > 1}
        print('# ', file=ot)
        print('# IPs in more than one family: {}'.format(len(ipsvar)), file=ot)
        for ip, f in sorted(ipsvar.items(), key=lambda x: -len(x[1])):
            print('#\t {}: {}'.format(ip, ', '.join(f)), file=ot)
        print(output, file=ot)

    with open(output_benign_seq, 'w') as ob:
        json_bs = [{k: list(v)} for k, v in benign_seq.items()]
        json.dump(json_bs, ob)

    if options.debug:
        print('Groups in the most common(20) list:')
        for label, total in Counter(labels).most_common(20):
            print('\t{}:\t{}'.format(label, total))

    if options.debug:
        print('Grouping by SNI (ESLD), done.')


if __name__ == '__main__':
    parser = optparse.OptionParser(
            usage="Usage: %prog [options] dataset_filename",
            version="%prog 1.0")
    parser.add_option(
            '-v', '--vectors', action='store', dest='vectors',
            help='Maximum number of vectors to process',
            type='int', default=10000)
    parser.add_option(
            '-b', '--benign', action='store', dest='benign',
            help='Filter benign connections using a specific CSV list',
            type=str, default='')
    parser.add_option(
            '-B', '--background-noise', action='store_true', dest='noise',
            help='Cluster background-noise connections (do not filter them)',
            default=False)
    parser.add_option(
            '-e', '--empty', action='store_true', dest='empty',
            help='Cluster vectors without payload (ApplicationData) packages',
            default=False)
    parser.add_option(
            '-t', '--tor', action='store_true', dest='tor',
            help='Group Tor-like connections into one single group',
            default=False)
    parser.add_option(
            '-D', '--debug', action='store_true', dest='debug',
            help='Print debug messages',
            default=False)

    options, args = parser.parse_args()

    if len(args) != 1 or not os.path.isfile(args[0]):
        parser.error("Dataset not found. Aborting...")

    main(options, args)
